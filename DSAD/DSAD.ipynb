{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/lukasruff/Deep-SAD-PyTorchm\n",
    "\n",
    "https://github.com/ZIYU-DEEP/deepSAD-custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_BN_leakyReLU(nn.Module):\n",
    "    \"\"\"\n",
    "    A nn.Module that consists of a Linear layer followed by BatchNorm1d and a leaky ReLu activation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=False, eps=1e-04):\n",
    "        super(Linear_BN_leakyReLU, self).__init__()\n",
    "\n",
    "        self.linear = nn.Linear(in_features, out_features, bias=bias)\n",
    "        self.bn = nn.BatchNorm1d(out_features, eps=eps, affine=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.leaky_relu(self.bn(self.linear(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \n",
    "    def __init__(self, x_dim, h_dims=[128, 64], rep_dim=32, bias=False):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.rep_dim = rep_dim # representation dimensionality, i.e, dim of the code layer or last layer.\n",
    "\n",
    "        neurons = [x_dim, *h_dims]\n",
    "        layers = [Linear_BN_leakyReLU(neurons[i - 1], neurons[i], bias=bias) for i in range(1, len(neurons))]\n",
    "\n",
    "        self.hidden = nn.ModuleList(layers)\n",
    "        self.code = nn.Linear(h_dims[-1], rep_dim, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(int(x.size(0)), -1)\n",
    "        for layer in self.hidden:\n",
    "            x = layer(x)\n",
    "        return self.code(x)\n",
    "\n",
    "\n",
    "class MLP_Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, x_dim, h_dims=[64, 128], rep_dim=32, bias=False):\n",
    "        super(MLP_Decoder, self).__init__()\n",
    "\n",
    "        self.rep_dim = rep_dim\n",
    "\n",
    "        neurons = [rep_dim, *h_dims]\n",
    "        layers = [Linear_BN_leakyReLU(neurons[i - 1], neurons[i], bias=bias) for i in range(1, len(neurons))]\n",
    "\n",
    "        self.hidden = nn.ModuleList(layers)\n",
    "        self.reconstruction = nn.Linear(h_dims[-1], x_dim, bias=bias)\n",
    "        self.output_activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(int(x.size(0)), -1)\n",
    "        for layer in self.hidden:\n",
    "            x = layer(x)\n",
    "        x = self.reconstruction(x)\n",
    "        return self.output_activation(x)\n",
    "\n",
    "\n",
    "class MLP_AE(nn.Module):\n",
    "\n",
    "    def __init__(self, x_dim, h_dims=[128, 64], rep_dim=32, bias=False):\n",
    "        super(MLP_AE, self).__init__()\n",
    "\n",
    "        self.rep_dim = rep_dim\n",
    "        self.encoder = MLP(x_dim, h_dims, rep_dim, bias)\n",
    "        self.decoder = MLP_Decoder(x_dim, list(reversed(h_dims)), rep_dim, bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_network(net_name, ae_net=None):\n",
    "    \"\"\"Builds the neural network.\"\"\"\n",
    "\n",
    "    implemented_networks = ('arrhythmia_mlp', 'cardio_mlp', 'satellite_mlp', 'satimage-2_mlp', 'shuttle_mlp',\n",
    "                            'thyroid_mlp')\n",
    "    assert net_name in implemented_networks\n",
    "\n",
    "    net = None\n",
    "\n",
    "    if net_name == 'arrhythmia_mlp':\n",
    "        net = MLP(x_dim=274, h_dims=[128, 64], rep_dim=32, bias=False)\n",
    "        \n",
    "    if net_name == 'thyroid_mlp':\n",
    "        net = MLP(x_dim=6, h_dims=[32, 16], rep_dim=4, bias=False)\n",
    "\n",
    "    # if net_name == 'cardio_mlp':\n",
    "    #     net = MLP(x_dim=21, h_dims=[32, 16], rep_dim=8, bias=False)\n",
    "    # if net_name == 'satellite_mlp':\n",
    "    #     net = MLP(x_dim=36, h_dims=[32, 16], rep_dim=8, bias=False)\n",
    "    # if net_name == 'satimage-2_mlp':\n",
    "    #     net = MLP(x_dim=36, h_dims=[32, 16], rep_dim=8, bias=False)\n",
    "    # if net_name == 'shuttle_mlp':\n",
    "    #     net = MLP(x_dim=9, h_dims=[32, 16], rep_dim=8, bias=False)\n",
    "\n",
    "    return net\n",
    "\n",
    "\n",
    "def build_autoencoder(net_name):\n",
    "    \"\"\"Builds the corresponding autoencoder network.\"\"\"\n",
    "\n",
    "    implemented_networks = ('arrhythmia_mlp', 'cardio_mlp', 'satellite_mlp', 'satimage-2_mlp', 'shuttle_mlp',\n",
    "                            'thyroid_mlp')\n",
    "\n",
    "    assert net_name in implemented_networks\n",
    "\n",
    "    ae_net = None\n",
    "\n",
    "    if net_name == 'arrhythmia_mlp':\n",
    "        ae_net = MLP_AE(x_dim=274, h_dims=[128, 64], rep_dim=32, bias=False)\n",
    "        \n",
    "    if net_name == 'thyroid_mlp':\n",
    "        ae_net = MLP_AE(x_dim=6, h_dims=[32, 16], rep_dim=4, bias=False)\n",
    "\n",
    "    # if net_name == 'cardio_mlp':\n",
    "    #     ae_net = MLP_AE(x_dim=21, h_dims=[32, 16], rep_dim=8, bias=False)\n",
    "    # if net_name == 'satellite_mlp':\n",
    "    #     ae_net = MLP_AE(x_dim=36, h_dims=[32, 16], rep_dim=8, bias=False)\n",
    "    # if net_name == 'satimage-2_mlp':\n",
    "    #     ae_net = MLP_AE(x_dim=36, h_dims=[32, 16], rep_dim=8, bias=False)\n",
    "    # if net_name == 'shuttle_mlp':\n",
    "    #     ae_net = MLP_AE(x_dim=9, h_dims=[32, 16], rep_dim=8, bias=False)\n",
    "\n",
    "    return ae_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dsad_trainer import AETrainer, DeepSADTrainer\n",
    "from dsad_dataset import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepSAD(object):\n",
    "    \"\"\"A class for the Deep SAD method.\n",
    "    Attributes:\n",
    "        eta: Deep SAD hyperparameter eta (must be 0 < eta).\n",
    "        c: Hypersphere center c.\n",
    "        net_name: A string indicating the name of the neural network to use.\n",
    "        net: The neural network phi.\n",
    "        trainer: DeepSADTrainer to train a Deep SAD model.\n",
    "        optimizer_name: A string indicating the optimizer to use for training the Deep SAD network.\n",
    "        ae_net: The autoencoder network corresponding to phi for network weights pretraining.\n",
    "        ae_trainer: AETrainer to train an autoencoder in pretraining.\n",
    "        ae_optimizer_name: A string indicating the optimizer to use for pretraining the autoencoder.\n",
    "        results: A dictionary to save the results.\n",
    "        ae_results: A dictionary to save the autoencoder results.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, eta = 1.0):\n",
    "        \"\"\"Inits DeepSAD with hyperparameter eta.\"\"\"\n",
    "\n",
    "        self.eta = eta\n",
    "        self.c = None  # hypersphere center c\n",
    "\n",
    "        self.net_name = None\n",
    "        self.net = None  # neural network phi\n",
    "\n",
    "        self.trainer = None\n",
    "        self.optimizer_name = None\n",
    "\n",
    "        self.ae_net = None  # autoencoder network for pretraining\n",
    "        self.ae_trainer = None\n",
    "        self.ae_optimizer_name = None\n",
    "\n",
    "        self.results = {\n",
    "            'train_time': None,\n",
    "            'test_auc': None,\n",
    "            'test_time': None,\n",
    "            'test_scores': None,\n",
    "        }\n",
    "\n",
    "        self.ae_results = {\n",
    "            'train_time': None,\n",
    "            'test_auc': None,\n",
    "            'test_time': None\n",
    "        }\n",
    "\n",
    "    def set_network(self, net_name):\n",
    "        \"\"\"Builds the neural network phi.\"\"\"\n",
    "        self.net_name = net_name\n",
    "        self.net = build_network(net_name)\n",
    "\n",
    "    def train(self, dataset, optimizer_name = 'adam', lr = 0.001, n_epochs = 50,\n",
    "              lr_milestones = (), batch_size = 128, weight_decay = 1e-6, device = 'cuda',\n",
    "              n_jobs_dataloader = 0):\n",
    "        \n",
    "        \"\"\"Trains the Deep SAD model on the training data.\"\"\"\n",
    "\n",
    "        self.optimizer_name = optimizer_name\n",
    "        self.trainer = DeepSADTrainer(self.c, self.eta, optimizer_name=optimizer_name, lr=lr, n_epochs=n_epochs,\n",
    "                                      lr_milestones=lr_milestones, batch_size=batch_size, weight_decay=weight_decay,\n",
    "                                      device=device, n_jobs_dataloader=n_jobs_dataloader)\n",
    "        # Get the model\n",
    "        self.net, self.loss_plot = self.trainer.train(dataset, self.net)\n",
    "        self.results['train_time'] = self.trainer.train_time\n",
    "        self.c = self.trainer.c.cpu().data.numpy().tolist()  # get as list\n",
    "\n",
    "    def test(self, dataset, device, n_jobs_dataloader):\n",
    "        \"\"\"Tests the Deep SAD model on the test data.\"\"\"\n",
    "\n",
    "        if self.trainer is None:\n",
    "            self.trainer, _ = DeepSADTrainer(self.c, self.eta, device=device, n_jobs_dataloader=n_jobs_dataloader)\n",
    "\n",
    "        self.trainer.test(dataset, self.net)\n",
    "\n",
    "        # Get results\n",
    "        self.results['test_auc'] = self.trainer.test_auc\n",
    "        self.results['test_time'] = self.trainer.test_time\n",
    "        self.results['test_scores'] = self.trainer.test_scores\n",
    "\n",
    "    def pretrain(self, dataset, optimizer_name = 'adam', lr = 0.001, n_epochs = 100,\n",
    "                 lr_milestones = (), batch_size = 128, weight_decay = 1e-6, device = 'cuda',\n",
    "                 n_jobs_dataloader = 0):\n",
    "        \"\"\"Pretrains the weights for the Deep SAD network phi via autoencoder.\"\"\"\n",
    "\n",
    "        # Set autoencoder network\n",
    "        self.ae_net = build_autoencoder(self.net_name)\n",
    "\n",
    "        # Train\n",
    "        self.ae_optimizer_name = optimizer_name\n",
    "        self.ae_trainer = AETrainer(optimizer_name, lr=lr, n_epochs=n_epochs, lr_milestones=lr_milestones,\n",
    "                                    batch_size=batch_size, weight_decay=weight_decay, device=device,\n",
    "                                    n_jobs_dataloader=n_jobs_dataloader)\n",
    "        self.ae_net, self.ae_loss_plot = self.ae_trainer.train(dataset, self.ae_net)\n",
    "\n",
    "        # Get train results\n",
    "        self.ae_results['train_time'] = self.ae_trainer.train_time\n",
    "\n",
    "        # Test\n",
    "        self.ae_trainer.test(dataset, self.ae_net)\n",
    "\n",
    "        # Get test results\n",
    "        self.ae_results['test_auc'] = self.ae_trainer.test_auc\n",
    "        self.ae_results['test_time'] = self.ae_trainer.test_time\n",
    "\n",
    "        # Initialize Deep SAD network weights from pre-trained encoder\n",
    "        self.init_network_weights_from_pretraining()\n",
    "\n",
    "    def init_network_weights_from_pretraining(self):\n",
    "        \"\"\"Initialize the Deep SAD network weights from the encoder weights of the pretraining autoencoder.\"\"\"\n",
    "\n",
    "        net_dict = self.net.state_dict()\n",
    "        ae_net_dict = self.ae_net.state_dict()\n",
    "\n",
    "        # Filter out decoder network keys\n",
    "        ae_net_dict = {k: v for k, v in ae_net_dict.items() if k in net_dict}\n",
    "        # Overwrite values in the existing state_dict\n",
    "        net_dict.update(ae_net_dict)\n",
    "        # Load the new state_dict\n",
    "        self.net.load_state_dict(net_dict)\n",
    "\n",
    "    def save_model(self, export_model, save_ae=True):\n",
    "        \"\"\"Save Deep SAD model to export_model.\"\"\"\n",
    "\n",
    "        net_dict = self.net.state_dict()\n",
    "        ae_net_dict = self.ae_net.state_dict() if save_ae else None\n",
    "\n",
    "        torch.save({'c': self.c,\n",
    "                    'net_dict': net_dict,\n",
    "                    'ae_net_dict': ae_net_dict}, export_model)\n",
    "\n",
    "    def load_model(self, model_path, load_ae=False, map_location='cpu'):\n",
    "        \"\"\"Load Deep SAD model from model_path.\"\"\"\n",
    "\n",
    "        model_dict = torch.load(model_path, map_location=map_location)\n",
    "\n",
    "        self.c = model_dict['c']\n",
    "        self.net.load_state_dict(model_dict['net_dict'])\n",
    "\n",
    "        # load autoencoder parameters if specified\n",
    "        if load_ae:\n",
    "            if self.ae_net is None:\n",
    "                self.ae_net = build_autoencoder(self.net_name)\n",
    "            self.ae_net.load_state_dict(model_dict['ae_net_dict'])\n",
    "\n",
    "    def save_results(self, export_json):\n",
    "        \"\"\"Save results dict to a JSON-file.\"\"\"\n",
    "        with open(export_json, 'w') as fp:\n",
    "            json.dump(self.results, fp)\n",
    "\n",
    "    def save_ae_results(self, export_json):\n",
    "        \"\"\"Save autoencoder results dict to a JSON-file.\"\"\"\n",
    "        with open(export_json, 'w') as fp:\n",
    "            json.dump(self.ae_results, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(dataset_name = 'arrhythmia', data_path = '/home/sewon/anomaly_detection/datasets', \n",
    "                       n_known_outlier_classes = 0, ratio_known_normal = 0.0, ratio_known_outlier = 0.0, \n",
    "                       ratio_pollution = 0.0, random_state = 413)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"eta\":1,\n",
    "    \"net_name\": 'arrhythmia_mlp',\n",
    "    \"ae_optimizer_name\": 'adam',\n",
    "    \"ae_lr\": 0.001,\n",
    "    \"ae_n_epochs\": 500,\n",
    "    \"ae_lr_milestones\": [50, 250, 500],\n",
    "    \"ae_batch_size\": 200,\n",
    "    \"ae_weight_decay\": 1e-4,\n",
    "    \"optimizer_name\": 'adam',\n",
    "    \"lr\": 0.001,\n",
    "    \"n_epochs\": 500,\n",
    "    \"lr_milestones\": [50, 250, 500],\n",
    "    \"batch_size\": 50,\n",
    "    \"weight_decay\": 1e-5,\n",
    "    \"n_jobs_dataloader\": 0,\n",
    "    \"export_path\": '/home/sewon/anomaly_detection/DSAD',\n",
    "    \"pretrain\": True,\n",
    "    \"num_threads\": 0    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Default device to 'cpu' if cuda is not available\n",
    "if not torch.cuda.is_available():\n",
    "    device = 'cpu'\n",
    "else:\n",
    "    device = 'cuda'\n",
    "# Set the number of threads used for parallelizing CPU operations\n",
    "num_threads = config['num_threads']\n",
    "if num_threads > 0:\n",
    "    torch.set_num_threads(num_threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DeepSAD model and set neural network phi\n",
    "deepSAD = DeepSAD(config['eta'])\n",
    "deepSAD.set_network(config['net_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting pretraining...\n",
      "| Epoch: 001/500 | Train Time: 0.010s | Train Loss: 0.166184 |\n",
      "| Epoch: 002/500 | Train Time: 0.005s | Train Loss: 0.157514 |\n",
      "| Epoch: 003/500 | Train Time: 0.005s | Train Loss: 0.149637 |\n",
      "| Epoch: 004/500 | Train Time: 0.005s | Train Loss: 0.142073 |\n",
      "| Epoch: 005/500 | Train Time: 0.005s | Train Loss: 0.134742 |\n",
      "| Epoch: 006/500 | Train Time: 0.005s | Train Loss: 0.127249 |\n",
      "| Epoch: 007/500 | Train Time: 0.005s | Train Loss: 0.119769 |\n",
      "| Epoch: 008/500 | Train Time: 0.005s | Train Loss: 0.113145 |\n",
      "| Epoch: 009/500 | Train Time: 0.005s | Train Loss: 0.106187 |\n",
      "| Epoch: 010/500 | Train Time: 0.005s | Train Loss: 0.099900 |\n",
      "| Epoch: 011/500 | Train Time: 0.005s | Train Loss: 0.093810 |\n",
      "| Epoch: 012/500 | Train Time: 0.005s | Train Loss: 0.088044 |\n",
      "| Epoch: 013/500 | Train Time: 0.005s | Train Loss: 0.082780 |\n",
      "| Epoch: 014/500 | Train Time: 0.005s | Train Loss: 0.077763 |\n",
      "| Epoch: 015/500 | Train Time: 0.005s | Train Loss: 0.072851 |\n",
      "| Epoch: 016/500 | Train Time: 0.005s | Train Loss: 0.068393 |\n",
      "| Epoch: 017/500 | Train Time: 0.005s | Train Loss: 0.064270 |\n",
      "| Epoch: 018/500 | Train Time: 0.005s | Train Loss: 0.060399 |\n",
      "| Epoch: 019/500 | Train Time: 0.005s | Train Loss: 0.056846 |\n",
      "| Epoch: 020/500 | Train Time: 0.005s | Train Loss: 0.053611 |\n",
      "| Epoch: 021/500 | Train Time: 0.005s | Train Loss: 0.050163 |\n",
      "| Epoch: 022/500 | Train Time: 0.005s | Train Loss: 0.047475 |\n",
      "| Epoch: 023/500 | Train Time: 0.005s | Train Loss: 0.044837 |\n",
      "| Epoch: 024/500 | Train Time: 0.005s | Train Loss: 0.042486 |\n",
      "| Epoch: 025/500 | Train Time: 0.005s | Train Loss: 0.039937 |\n",
      "| Epoch: 026/500 | Train Time: 0.005s | Train Loss: 0.038044 |\n",
      "| Epoch: 027/500 | Train Time: 0.005s | Train Loss: 0.036195 |\n",
      "| Epoch: 028/500 | Train Time: 0.005s | Train Loss: 0.034171 |\n",
      "| Epoch: 029/500 | Train Time: 0.005s | Train Loss: 0.032661 |\n",
      "| Epoch: 030/500 | Train Time: 0.005s | Train Loss: 0.031320 |\n",
      "| Epoch: 031/500 | Train Time: 0.005s | Train Loss: 0.029952 |\n",
      "| Epoch: 032/500 | Train Time: 0.005s | Train Loss: 0.028747 |\n",
      "| Epoch: 033/500 | Train Time: 0.005s | Train Loss: 0.027485 |\n",
      "| Epoch: 034/500 | Train Time: 0.004s | Train Loss: 0.026480 |\n",
      "| Epoch: 035/500 | Train Time: 0.004s | Train Loss: 0.025608 |\n",
      "| Epoch: 036/500 | Train Time: 0.005s | Train Loss: 0.024593 |\n",
      "| Epoch: 037/500 | Train Time: 0.005s | Train Loss: 0.023407 |\n",
      "| Epoch: 038/500 | Train Time: 0.005s | Train Loss: 0.022954 |\n",
      "| Epoch: 039/500 | Train Time: 0.004s | Train Loss: 0.022273 |\n",
      "| Epoch: 040/500 | Train Time: 0.005s | Train Loss: 0.021373 |\n",
      "| Epoch: 041/500 | Train Time: 0.006s | Train Loss: 0.020866 |\n",
      "| Epoch: 042/500 | Train Time: 0.005s | Train Loss: 0.020426 |\n",
      "| Epoch: 043/500 | Train Time: 0.004s | Train Loss: 0.019862 |\n",
      "| Epoch: 044/500 | Train Time: 0.004s | Train Loss: 0.019044 |\n",
      "| Epoch: 045/500 | Train Time: 0.004s | Train Loss: 0.018726 |\n",
      "| Epoch: 046/500 | Train Time: 0.005s | Train Loss: 0.018237 |\n",
      "| Epoch: 047/500 | Train Time: 0.004s | Train Loss: 0.017871 |\n",
      "| Epoch: 048/500 | Train Time: 0.004s | Train Loss: 0.017836 |\n",
      "| Epoch: 049/500 | Train Time: 0.004s | Train Loss: 0.016906 |\n",
      "| Epoch: 050/500 | Train Time: 0.005s | Train Loss: 0.017015 |\n",
      "  LR scheduler: new learning rate is 1e-05\n",
      "| Epoch: 051/500 | Train Time: 0.005s | Train Loss: 0.016766 |\n",
      "| Epoch: 052/500 | Train Time: 0.005s | Train Loss: 0.016606 |\n",
      "| Epoch: 053/500 | Train Time: 0.005s | Train Loss: 0.016357 |\n",
      "| Epoch: 054/500 | Train Time: 0.004s | Train Loss: 0.016512 |\n",
      "| Epoch: 055/500 | Train Time: 0.004s | Train Loss: 0.016369 |\n",
      "| Epoch: 056/500 | Train Time: 0.004s | Train Loss: 0.016609 |\n",
      "| Epoch: 057/500 | Train Time: 0.005s | Train Loss: 0.016327 |\n",
      "| Epoch: 058/500 | Train Time: 0.004s | Train Loss: 0.016078 |\n",
      "| Epoch: 059/500 | Train Time: 0.004s | Train Loss: 0.016562 |\n",
      "| Epoch: 060/500 | Train Time: 0.004s | Train Loss: 0.016271 |\n",
      "| Epoch: 061/500 | Train Time: 0.005s | Train Loss: 0.016399 |\n",
      "| Epoch: 062/500 | Train Time: 0.005s | Train Loss: 0.016295 |\n",
      "| Epoch: 063/500 | Train Time: 0.004s | Train Loss: 0.016407 |\n",
      "| Epoch: 064/500 | Train Time: 0.005s | Train Loss: 0.016027 |\n",
      "| Epoch: 065/500 | Train Time: 0.004s | Train Loss: 0.016184 |\n",
      "| Epoch: 066/500 | Train Time: 0.005s | Train Loss: 0.015987 |\n",
      "| Epoch: 067/500 | Train Time: 0.005s | Train Loss: 0.016345 |\n",
      "| Epoch: 068/500 | Train Time: 0.005s | Train Loss: 0.016146 |\n",
      "| Epoch: 069/500 | Train Time: 0.005s | Train Loss: 0.016352 |\n",
      "| Epoch: 070/500 | Train Time: 0.005s | Train Loss: 0.016221 |\n",
      "| Epoch: 071/500 | Train Time: 0.005s | Train Loss: 0.016079 |\n",
      "| Epoch: 072/500 | Train Time: 0.005s | Train Loss: 0.016343 |\n",
      "| Epoch: 073/500 | Train Time: 0.005s | Train Loss: 0.016024 |\n",
      "| Epoch: 074/500 | Train Time: 0.005s | Train Loss: 0.015852 |\n",
      "| Epoch: 075/500 | Train Time: 0.005s | Train Loss: 0.015786 |\n",
      "| Epoch: 076/500 | Train Time: 0.005s | Train Loss: 0.016208 |\n",
      "| Epoch: 077/500 | Train Time: 0.005s | Train Loss: 0.016015 |\n",
      "| Epoch: 078/500 | Train Time: 0.005s | Train Loss: 0.016101 |\n",
      "| Epoch: 079/500 | Train Time: 0.005s | Train Loss: 0.015955 |\n",
      "| Epoch: 080/500 | Train Time: 0.004s | Train Loss: 0.015728 |\n",
      "| Epoch: 081/500 | Train Time: 0.004s | Train Loss: 0.016005 |\n",
      "| Epoch: 082/500 | Train Time: 0.004s | Train Loss: 0.015928 |\n",
      "| Epoch: 083/500 | Train Time: 0.004s | Train Loss: 0.015697 |\n",
      "| Epoch: 084/500 | Train Time: 0.005s | Train Loss: 0.015644 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sewon/anaconda3/envs/venvs/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:418: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 085/500 | Train Time: 0.006s | Train Loss: 0.015588 |\n",
      "| Epoch: 086/500 | Train Time: 0.004s | Train Loss: 0.015892 |\n",
      "| Epoch: 087/500 | Train Time: 0.005s | Train Loss: 0.015860 |\n",
      "| Epoch: 088/500 | Train Time: 0.004s | Train Loss: 0.015639 |\n",
      "| Epoch: 089/500 | Train Time: 0.004s | Train Loss: 0.015617 |\n",
      "| Epoch: 090/500 | Train Time: 0.004s | Train Loss: 0.015757 |\n",
      "| Epoch: 091/500 | Train Time: 0.004s | Train Loss: 0.015666 |\n",
      "| Epoch: 092/500 | Train Time: 0.004s | Train Loss: 0.015721 |\n",
      "| Epoch: 093/500 | Train Time: 0.004s | Train Loss: 0.015499 |\n",
      "| Epoch: 094/500 | Train Time: 0.005s | Train Loss: 0.015698 |\n",
      "| Epoch: 095/500 | Train Time: 0.004s | Train Loss: 0.015642 |\n",
      "| Epoch: 096/500 | Train Time: 0.005s | Train Loss: 0.015383 |\n",
      "| Epoch: 097/500 | Train Time: 0.005s | Train Loss: 0.015509 |\n",
      "| Epoch: 098/500 | Train Time: 0.004s | Train Loss: 0.015524 |\n",
      "| Epoch: 099/500 | Train Time: 0.005s | Train Loss: 0.015443 |\n",
      "| Epoch: 100/500 | Train Time: 0.004s | Train Loss: 0.015329 |\n",
      "| Epoch: 101/500 | Train Time: 0.005s | Train Loss: 0.015249 |\n",
      "| Epoch: 102/500 | Train Time: 0.004s | Train Loss: 0.015501 |\n",
      "| Epoch: 103/500 | Train Time: 0.004s | Train Loss: 0.015616 |\n",
      "| Epoch: 104/500 | Train Time: 0.004s | Train Loss: 0.014772 |\n",
      "| Epoch: 105/500 | Train Time: 0.005s | Train Loss: 0.015251 |\n",
      "| Epoch: 106/500 | Train Time: 0.004s | Train Loss: 0.015380 |\n",
      "| Epoch: 107/500 | Train Time: 0.005s | Train Loss: 0.015268 |\n",
      "| Epoch: 108/500 | Train Time: 0.005s | Train Loss: 0.015470 |\n",
      "| Epoch: 109/500 | Train Time: 0.004s | Train Loss: 0.014902 |\n",
      "| Epoch: 110/500 | Train Time: 0.004s | Train Loss: 0.015271 |\n",
      "| Epoch: 111/500 | Train Time: 0.004s | Train Loss: 0.015176 |\n",
      "| Epoch: 112/500 | Train Time: 0.005s | Train Loss: 0.014744 |\n",
      "| Epoch: 113/500 | Train Time: 0.004s | Train Loss: 0.015159 |\n",
      "| Epoch: 114/500 | Train Time: 0.004s | Train Loss: 0.014934 |\n",
      "| Epoch: 115/500 | Train Time: 0.004s | Train Loss: 0.014978 |\n",
      "| Epoch: 116/500 | Train Time: 0.004s | Train Loss: 0.015231 |\n",
      "| Epoch: 117/500 | Train Time: 0.005s | Train Loss: 0.014835 |\n",
      "| Epoch: 118/500 | Train Time: 0.005s | Train Loss: 0.015156 |\n",
      "| Epoch: 119/500 | Train Time: 0.005s | Train Loss: 0.014767 |\n",
      "| Epoch: 120/500 | Train Time: 0.004s | Train Loss: 0.015051 |\n",
      "| Epoch: 121/500 | Train Time: 0.004s | Train Loss: 0.015188 |\n",
      "| Epoch: 122/500 | Train Time: 0.004s | Train Loss: 0.015061 |\n",
      "| Epoch: 123/500 | Train Time: 0.005s | Train Loss: 0.015157 |\n",
      "| Epoch: 124/500 | Train Time: 0.005s | Train Loss: 0.014773 |\n",
      "| Epoch: 125/500 | Train Time: 0.004s | Train Loss: 0.014696 |\n",
      "| Epoch: 126/500 | Train Time: 0.004s | Train Loss: 0.015022 |\n",
      "| Epoch: 127/500 | Train Time: 0.005s | Train Loss: 0.014817 |\n",
      "| Epoch: 128/500 | Train Time: 0.005s | Train Loss: 0.015025 |\n",
      "| Epoch: 129/500 | Train Time: 0.004s | Train Loss: 0.014755 |\n",
      "| Epoch: 130/500 | Train Time: 0.006s | Train Loss: 0.014674 |\n",
      "| Epoch: 131/500 | Train Time: 0.005s | Train Loss: 0.014766 |\n",
      "| Epoch: 132/500 | Train Time: 0.004s | Train Loss: 0.014828 |\n",
      "| Epoch: 133/500 | Train Time: 0.004s | Train Loss: 0.014825 |\n",
      "| Epoch: 134/500 | Train Time: 0.004s | Train Loss: 0.014736 |\n",
      "| Epoch: 135/500 | Train Time: 0.005s | Train Loss: 0.014256 |\n",
      "| Epoch: 136/500 | Train Time: 0.005s | Train Loss: 0.014633 |\n",
      "| Epoch: 137/500 | Train Time: 0.004s | Train Loss: 0.014353 |\n",
      "| Epoch: 138/500 | Train Time: 0.005s | Train Loss: 0.014923 |\n",
      "| Epoch: 139/500 | Train Time: 0.005s | Train Loss: 0.014854 |\n",
      "| Epoch: 140/500 | Train Time: 0.005s | Train Loss: 0.014352 |\n",
      "| Epoch: 141/500 | Train Time: 0.005s | Train Loss: 0.014422 |\n",
      "| Epoch: 142/500 | Train Time: 0.004s | Train Loss: 0.014712 |\n",
      "| Epoch: 143/500 | Train Time: 0.004s | Train Loss: 0.014395 |\n",
      "| Epoch: 144/500 | Train Time: 0.005s | Train Loss: 0.014618 |\n",
      "| Epoch: 145/500 | Train Time: 0.005s | Train Loss: 0.014581 |\n",
      "| Epoch: 146/500 | Train Time: 0.005s | Train Loss: 0.014428 |\n",
      "| Epoch: 147/500 | Train Time: 0.004s | Train Loss: 0.014185 |\n",
      "| Epoch: 148/500 | Train Time: 0.005s | Train Loss: 0.014632 |\n",
      "| Epoch: 149/500 | Train Time: 0.005s | Train Loss: 0.014286 |\n",
      "| Epoch: 150/500 | Train Time: 0.005s | Train Loss: 0.014462 |\n",
      "| Epoch: 151/500 | Train Time: 0.005s | Train Loss: 0.014108 |\n",
      "| Epoch: 152/500 | Train Time: 0.005s | Train Loss: 0.014422 |\n",
      "| Epoch: 153/500 | Train Time: 0.005s | Train Loss: 0.014063 |\n",
      "| Epoch: 154/500 | Train Time: 0.004s | Train Loss: 0.014329 |\n",
      "| Epoch: 155/500 | Train Time: 0.005s | Train Loss: 0.014339 |\n",
      "| Epoch: 156/500 | Train Time: 0.005s | Train Loss: 0.014441 |\n",
      "| Epoch: 157/500 | Train Time: 0.005s | Train Loss: 0.014344 |\n",
      "| Epoch: 158/500 | Train Time: 0.004s | Train Loss: 0.014333 |\n",
      "| Epoch: 159/500 | Train Time: 0.005s | Train Loss: 0.013872 |\n",
      "| Epoch: 160/500 | Train Time: 0.005s | Train Loss: 0.014335 |\n",
      "| Epoch: 161/500 | Train Time: 0.004s | Train Loss: 0.014313 |\n",
      "| Epoch: 162/500 | Train Time: 0.005s | Train Loss: 0.014031 |\n",
      "| Epoch: 163/500 | Train Time: 0.005s | Train Loss: 0.014070 |\n",
      "| Epoch: 164/500 | Train Time: 0.005s | Train Loss: 0.013916 |\n",
      "| Epoch: 165/500 | Train Time: 0.005s | Train Loss: 0.014078 |\n",
      "| Epoch: 166/500 | Train Time: 0.005s | Train Loss: 0.014061 |\n",
      "| Epoch: 167/500 | Train Time: 0.005s | Train Loss: 0.013937 |\n",
      "| Epoch: 168/500 | Train Time: 0.004s | Train Loss: 0.014157 |\n",
      "| Epoch: 169/500 | Train Time: 0.005s | Train Loss: 0.013905 |\n",
      "| Epoch: 170/500 | Train Time: 0.005s | Train Loss: 0.014179 |\n",
      "| Epoch: 171/500 | Train Time: 0.005s | Train Loss: 0.014031 |\n",
      "| Epoch: 172/500 | Train Time: 0.005s | Train Loss: 0.014134 |\n",
      "| Epoch: 173/500 | Train Time: 0.005s | Train Loss: 0.014078 |\n",
      "| Epoch: 174/500 | Train Time: 0.006s | Train Loss: 0.013906 |\n",
      "| Epoch: 175/500 | Train Time: 0.005s | Train Loss: 0.013838 |\n",
      "| Epoch: 176/500 | Train Time: 0.004s | Train Loss: 0.013823 |\n",
      "| Epoch: 177/500 | Train Time: 0.005s | Train Loss: 0.013824 |\n",
      "| Epoch: 178/500 | Train Time: 0.005s | Train Loss: 0.014051 |\n",
      "| Epoch: 179/500 | Train Time: 0.004s | Train Loss: 0.013519 |\n",
      "| Epoch: 180/500 | Train Time: 0.004s | Train Loss: 0.013865 |\n",
      "| Epoch: 181/500 | Train Time: 0.004s | Train Loss: 0.013925 |\n",
      "| Epoch: 182/500 | Train Time: 0.004s | Train Loss: 0.013807 |\n",
      "| Epoch: 183/500 | Train Time: 0.004s | Train Loss: 0.014060 |\n",
      "| Epoch: 184/500 | Train Time: 0.005s | Train Loss: 0.013653 |\n",
      "| Epoch: 185/500 | Train Time: 0.004s | Train Loss: 0.013533 |\n",
      "| Epoch: 186/500 | Train Time: 0.004s | Train Loss: 0.013847 |\n",
      "| Epoch: 187/500 | Train Time: 0.004s | Train Loss: 0.013485 |\n",
      "| Epoch: 188/500 | Train Time: 0.005s | Train Loss: 0.013456 |\n",
      "| Epoch: 189/500 | Train Time: 0.005s | Train Loss: 0.013457 |\n",
      "| Epoch: 190/500 | Train Time: 0.005s | Train Loss: 0.013779 |\n",
      "| Epoch: 191/500 | Train Time: 0.004s | Train Loss: 0.013869 |\n",
      "| Epoch: 192/500 | Train Time: 0.004s | Train Loss: 0.013683 |\n",
      "| Epoch: 193/500 | Train Time: 0.005s | Train Loss: 0.013679 |\n",
      "| Epoch: 194/500 | Train Time: 0.004s | Train Loss: 0.013644 |\n",
      "| Epoch: 195/500 | Train Time: 0.005s | Train Loss: 0.013593 |\n",
      "| Epoch: 196/500 | Train Time: 0.005s | Train Loss: 0.013650 |\n",
      "| Epoch: 197/500 | Train Time: 0.004s | Train Loss: 0.013365 |\n",
      "| Epoch: 198/500 | Train Time: 0.004s | Train Loss: 0.013283 |\n",
      "| Epoch: 199/500 | Train Time: 0.004s | Train Loss: 0.013571 |\n",
      "| Epoch: 200/500 | Train Time: 0.005s | Train Loss: 0.013616 |\n",
      "| Epoch: 201/500 | Train Time: 0.005s | Train Loss: 0.013220 |\n",
      "| Epoch: 202/500 | Train Time: 0.004s | Train Loss: 0.013480 |\n",
      "| Epoch: 203/500 | Train Time: 0.005s | Train Loss: 0.013632 |\n",
      "| Epoch: 204/500 | Train Time: 0.005s | Train Loss: 0.013480 |\n",
      "| Epoch: 205/500 | Train Time: 0.005s | Train Loss: 0.013336 |\n",
      "| Epoch: 206/500 | Train Time: 0.005s | Train Loss: 0.013075 |\n",
      "| Epoch: 207/500 | Train Time: 0.005s | Train Loss: 0.013328 |\n",
      "| Epoch: 208/500 | Train Time: 0.004s | Train Loss: 0.013160 |\n",
      "| Epoch: 209/500 | Train Time: 0.005s | Train Loss: 0.013276 |\n",
      "| Epoch: 210/500 | Train Time: 0.005s | Train Loss: 0.013031 |\n",
      "| Epoch: 211/500 | Train Time: 0.005s | Train Loss: 0.013257 |\n",
      "| Epoch: 212/500 | Train Time: 0.005s | Train Loss: 0.012966 |\n",
      "| Epoch: 213/500 | Train Time: 0.005s | Train Loss: 0.013486 |\n",
      "| Epoch: 214/500 | Train Time: 0.005s | Train Loss: 0.013083 |\n",
      "| Epoch: 215/500 | Train Time: 0.005s | Train Loss: 0.013278 |\n",
      "| Epoch: 216/500 | Train Time: 0.005s | Train Loss: 0.013314 |\n",
      "| Epoch: 217/500 | Train Time: 0.005s | Train Loss: 0.013200 |\n",
      "| Epoch: 218/500 | Train Time: 0.006s | Train Loss: 0.013287 |\n",
      "| Epoch: 219/500 | Train Time: 0.005s | Train Loss: 0.012993 |\n",
      "| Epoch: 220/500 | Train Time: 0.005s | Train Loss: 0.013133 |\n",
      "| Epoch: 221/500 | Train Time: 0.005s | Train Loss: 0.013254 |\n",
      "| Epoch: 222/500 | Train Time: 0.005s | Train Loss: 0.013067 |\n",
      "| Epoch: 223/500 | Train Time: 0.005s | Train Loss: 0.012965 |\n",
      "| Epoch: 224/500 | Train Time: 0.005s | Train Loss: 0.012747 |\n",
      "| Epoch: 225/500 | Train Time: 0.005s | Train Loss: 0.012918 |\n",
      "| Epoch: 226/500 | Train Time: 0.005s | Train Loss: 0.013275 |\n",
      "| Epoch: 227/500 | Train Time: 0.005s | Train Loss: 0.013058 |\n",
      "| Epoch: 228/500 | Train Time: 0.004s | Train Loss: 0.012971 |\n",
      "| Epoch: 229/500 | Train Time: 0.004s | Train Loss: 0.013141 |\n",
      "| Epoch: 230/500 | Train Time: 0.005s | Train Loss: 0.012825 |\n",
      "| Epoch: 231/500 | Train Time: 0.005s | Train Loss: 0.012894 |\n",
      "| Epoch: 232/500 | Train Time: 0.005s | Train Loss: 0.012958 |\n",
      "| Epoch: 233/500 | Train Time: 0.005s | Train Loss: 0.012857 |\n",
      "| Epoch: 234/500 | Train Time: 0.004s | Train Loss: 0.012575 |\n",
      "| Epoch: 235/500 | Train Time: 0.005s | Train Loss: 0.012690 |\n",
      "| Epoch: 236/500 | Train Time: 0.005s | Train Loss: 0.012535 |\n",
      "| Epoch: 237/500 | Train Time: 0.005s | Train Loss: 0.012332 |\n",
      "| Epoch: 238/500 | Train Time: 0.005s | Train Loss: 0.012790 |\n",
      "| Epoch: 239/500 | Train Time: 0.005s | Train Loss: 0.012713 |\n",
      "| Epoch: 240/500 | Train Time: 0.004s | Train Loss: 0.012497 |\n",
      "| Epoch: 241/500 | Train Time: 0.004s | Train Loss: 0.012851 |\n",
      "| Epoch: 242/500 | Train Time: 0.005s | Train Loss: 0.012582 |\n",
      "| Epoch: 243/500 | Train Time: 0.005s | Train Loss: 0.012685 |\n",
      "| Epoch: 244/500 | Train Time: 0.004s | Train Loss: 0.012666 |\n",
      "| Epoch: 245/500 | Train Time: 0.005s | Train Loss: 0.012680 |\n",
      "| Epoch: 246/500 | Train Time: 0.004s | Train Loss: 0.012585 |\n",
      "| Epoch: 247/500 | Train Time: 0.005s | Train Loss: 0.012613 |\n",
      "| Epoch: 248/500 | Train Time: 0.005s | Train Loss: 0.012854 |\n",
      "| Epoch: 249/500 | Train Time: 0.005s | Train Loss: 0.012985 |\n",
      "| Epoch: 250/500 | Train Time: 0.004s | Train Loss: 0.012626 |\n",
      "  LR scheduler: new learning rate is 1e-06\n",
      "| Epoch: 251/500 | Train Time: 0.004s | Train Loss: 0.012587 |\n",
      "| Epoch: 252/500 | Train Time: 0.005s | Train Loss: 0.012543 |\n",
      "| Epoch: 253/500 | Train Time: 0.005s | Train Loss: 0.012718 |\n",
      "| Epoch: 254/500 | Train Time: 0.004s | Train Loss: 0.012422 |\n",
      "| Epoch: 255/500 | Train Time: 0.004s | Train Loss: 0.012580 |\n",
      "| Epoch: 256/500 | Train Time: 0.004s | Train Loss: 0.012709 |\n",
      "| Epoch: 257/500 | Train Time: 0.005s | Train Loss: 0.012539 |\n",
      "| Epoch: 258/500 | Train Time: 0.005s | Train Loss: 0.012691 |\n",
      "| Epoch: 259/500 | Train Time: 0.005s | Train Loss: 0.012781 |\n",
      "| Epoch: 260/500 | Train Time: 0.005s | Train Loss: 0.012721 |\n",
      "| Epoch: 261/500 | Train Time: 0.004s | Train Loss: 0.012837 |\n",
      "| Epoch: 262/500 | Train Time: 0.005s | Train Loss: 0.012174 |\n",
      "| Epoch: 263/500 | Train Time: 0.005s | Train Loss: 0.012478 |\n",
      "| Epoch: 264/500 | Train Time: 0.004s | Train Loss: 0.012600 |\n",
      "| Epoch: 265/500 | Train Time: 0.004s | Train Loss: 0.012373 |\n",
      "| Epoch: 266/500 | Train Time: 0.005s | Train Loss: 0.012684 |\n",
      "| Epoch: 267/500 | Train Time: 0.004s | Train Loss: 0.012628 |\n",
      "| Epoch: 268/500 | Train Time: 0.004s | Train Loss: 0.012862 |\n",
      "| Epoch: 269/500 | Train Time: 0.005s | Train Loss: 0.012295 |\n",
      "| Epoch: 270/500 | Train Time: 0.004s | Train Loss: 0.012609 |\n",
      "| Epoch: 271/500 | Train Time: 0.005s | Train Loss: 0.012639 |\n",
      "| Epoch: 272/500 | Train Time: 0.004s | Train Loss: 0.012751 |\n",
      "| Epoch: 273/500 | Train Time: 0.004s | Train Loss: 0.012659 |\n",
      "| Epoch: 274/500 | Train Time: 0.004s | Train Loss: 0.012596 |\n",
      "| Epoch: 275/500 | Train Time: 0.004s | Train Loss: 0.012538 |\n",
      "| Epoch: 276/500 | Train Time: 0.005s | Train Loss: 0.012177 |\n",
      "| Epoch: 277/500 | Train Time: 0.004s | Train Loss: 0.012581 |\n",
      "| Epoch: 278/500 | Train Time: 0.004s | Train Loss: 0.012695 |\n",
      "| Epoch: 279/500 | Train Time: 0.004s | Train Loss: 0.012188 |\n",
      "| Epoch: 280/500 | Train Time: 0.004s | Train Loss: 0.012781 |\n",
      "| Epoch: 281/500 | Train Time: 0.004s | Train Loss: 0.012883 |\n",
      "| Epoch: 282/500 | Train Time: 0.005s | Train Loss: 0.012674 |\n",
      "| Epoch: 283/500 | Train Time: 0.005s | Train Loss: 0.012653 |\n",
      "| Epoch: 284/500 | Train Time: 0.004s | Train Loss: 0.011992 |\n",
      "| Epoch: 285/500 | Train Time: 0.004s | Train Loss: 0.012555 |\n",
      "| Epoch: 286/500 | Train Time: 0.005s | Train Loss: 0.012682 |\n",
      "| Epoch: 287/500 | Train Time: 0.005s | Train Loss: 0.012706 |\n",
      "| Epoch: 288/500 | Train Time: 0.005s | Train Loss: 0.012508 |\n",
      "| Epoch: 289/500 | Train Time: 0.005s | Train Loss: 0.012751 |\n",
      "| Epoch: 290/500 | Train Time: 0.005s | Train Loss: 0.012443 |\n",
      "| Epoch: 291/500 | Train Time: 0.004s | Train Loss: 0.012644 |\n",
      "| Epoch: 292/500 | Train Time: 0.004s | Train Loss: 0.012600 |\n",
      "| Epoch: 293/500 | Train Time: 0.004s | Train Loss: 0.012225 |\n",
      "| Epoch: 294/500 | Train Time: 0.005s | Train Loss: 0.012292 |\n",
      "| Epoch: 295/500 | Train Time: 0.004s | Train Loss: 0.012198 |\n",
      "| Epoch: 296/500 | Train Time: 0.004s | Train Loss: 0.012731 |\n",
      "| Epoch: 297/500 | Train Time: 0.004s | Train Loss: 0.012655 |\n",
      "| Epoch: 298/500 | Train Time: 0.005s | Train Loss: 0.012775 |\n",
      "| Epoch: 299/500 | Train Time: 0.004s | Train Loss: 0.012315 |\n",
      "| Epoch: 300/500 | Train Time: 0.004s | Train Loss: 0.012625 |\n",
      "| Epoch: 301/500 | Train Time: 0.004s | Train Loss: 0.012613 |\n",
      "| Epoch: 302/500 | Train Time: 0.004s | Train Loss: 0.012353 |\n",
      "| Epoch: 303/500 | Train Time: 0.004s | Train Loss: 0.012172 |\n",
      "| Epoch: 304/500 | Train Time: 0.004s | Train Loss: 0.012347 |\n",
      "| Epoch: 305/500 | Train Time: 0.004s | Train Loss: 0.012552 |\n",
      "| Epoch: 306/500 | Train Time: 0.004s | Train Loss: 0.012732 |\n",
      "| Epoch: 307/500 | Train Time: 0.004s | Train Loss: 0.012716 |\n",
      "| Epoch: 308/500 | Train Time: 0.005s | Train Loss: 0.012333 |\n",
      "| Epoch: 309/500 | Train Time: 0.004s | Train Loss: 0.012502 |\n",
      "| Epoch: 310/500 | Train Time: 0.004s | Train Loss: 0.012707 |\n",
      "| Epoch: 311/500 | Train Time: 0.004s | Train Loss: 0.012677 |\n",
      "| Epoch: 312/500 | Train Time: 0.004s | Train Loss: 0.012443 |\n",
      "| Epoch: 313/500 | Train Time: 0.004s | Train Loss: 0.012672 |\n",
      "| Epoch: 314/500 | Train Time: 0.004s | Train Loss: 0.012568 |\n",
      "| Epoch: 315/500 | Train Time: 0.004s | Train Loss: 0.012518 |\n",
      "| Epoch: 316/500 | Train Time: 0.004s | Train Loss: 0.012598 |\n",
      "| Epoch: 317/500 | Train Time: 0.004s | Train Loss: 0.012266 |\n",
      "| Epoch: 318/500 | Train Time: 0.004s | Train Loss: 0.012468 |\n",
      "| Epoch: 319/500 | Train Time: 0.004s | Train Loss: 0.012379 |\n",
      "| Epoch: 320/500 | Train Time: 0.004s | Train Loss: 0.012596 |\n",
      "| Epoch: 321/500 | Train Time: 0.004s | Train Loss: 0.012579 |\n",
      "| Epoch: 322/500 | Train Time: 0.004s | Train Loss: 0.012749 |\n",
      "| Epoch: 323/500 | Train Time: 0.004s | Train Loss: 0.012622 |\n",
      "| Epoch: 324/500 | Train Time: 0.004s | Train Loss: 0.012590 |\n",
      "| Epoch: 325/500 | Train Time: 0.004s | Train Loss: 0.012617 |\n",
      "| Epoch: 326/500 | Train Time: 0.004s | Train Loss: 0.012298 |\n",
      "| Epoch: 327/500 | Train Time: 0.004s | Train Loss: 0.012604 |\n",
      "| Epoch: 328/500 | Train Time: 0.004s | Train Loss: 0.012493 |\n",
      "| Epoch: 329/500 | Train Time: 0.004s | Train Loss: 0.012033 |\n",
      "| Epoch: 330/500 | Train Time: 0.004s | Train Loss: 0.012524 |\n",
      "| Epoch: 331/500 | Train Time: 0.004s | Train Loss: 0.012374 |\n",
      "| Epoch: 332/500 | Train Time: 0.004s | Train Loss: 0.012618 |\n",
      "| Epoch: 333/500 | Train Time: 0.004s | Train Loss: 0.012455 |\n",
      "| Epoch: 334/500 | Train Time: 0.004s | Train Loss: 0.012504 |\n",
      "| Epoch: 335/500 | Train Time: 0.004s | Train Loss: 0.012480 |\n",
      "| Epoch: 336/500 | Train Time: 0.004s | Train Loss: 0.012205 |\n",
      "| Epoch: 337/500 | Train Time: 0.004s | Train Loss: 0.012542 |\n",
      "| Epoch: 338/500 | Train Time: 0.004s | Train Loss: 0.012248 |\n",
      "| Epoch: 339/500 | Train Time: 0.004s | Train Loss: 0.012199 |\n",
      "| Epoch: 340/500 | Train Time: 0.004s | Train Loss: 0.012644 |\n",
      "| Epoch: 341/500 | Train Time: 0.004s | Train Loss: 0.012523 |\n",
      "| Epoch: 342/500 | Train Time: 0.004s | Train Loss: 0.012466 |\n",
      "| Epoch: 343/500 | Train Time: 0.004s | Train Loss: 0.012692 |\n",
      "| Epoch: 344/500 | Train Time: 0.004s | Train Loss: 0.012365 |\n",
      "| Epoch: 345/500 | Train Time: 0.004s | Train Loss: 0.012693 |\n",
      "| Epoch: 346/500 | Train Time: 0.004s | Train Loss: 0.012385 |\n",
      "| Epoch: 347/500 | Train Time: 0.004s | Train Loss: 0.012422 |\n",
      "| Epoch: 348/500 | Train Time: 0.004s | Train Loss: 0.012300 |\n",
      "| Epoch: 349/500 | Train Time: 0.004s | Train Loss: 0.012549 |\n",
      "| Epoch: 350/500 | Train Time: 0.004s | Train Loss: 0.012336 |\n",
      "| Epoch: 351/500 | Train Time: 0.004s | Train Loss: 0.012633 |\n",
      "| Epoch: 352/500 | Train Time: 0.004s | Train Loss: 0.012494 |\n",
      "| Epoch: 353/500 | Train Time: 0.004s | Train Loss: 0.012448 |\n",
      "| Epoch: 354/500 | Train Time: 0.004s | Train Loss: 0.012358 |\n",
      "| Epoch: 355/500 | Train Time: 0.004s | Train Loss: 0.012480 |\n",
      "| Epoch: 356/500 | Train Time: 0.004s | Train Loss: 0.012501 |\n",
      "| Epoch: 357/500 | Train Time: 0.004s | Train Loss: 0.012281 |\n",
      "| Epoch: 358/500 | Train Time: 0.005s | Train Loss: 0.012560 |\n",
      "| Epoch: 359/500 | Train Time: 0.004s | Train Loss: 0.012617 |\n",
      "| Epoch: 360/500 | Train Time: 0.005s | Train Loss: 0.012557 |\n",
      "| Epoch: 361/500 | Train Time: 0.004s | Train Loss: 0.012110 |\n",
      "| Epoch: 362/500 | Train Time: 0.004s | Train Loss: 0.012385 |\n",
      "| Epoch: 363/500 | Train Time: 0.004s | Train Loss: 0.012493 |\n",
      "| Epoch: 364/500 | Train Time: 0.004s | Train Loss: 0.012438 |\n",
      "| Epoch: 365/500 | Train Time: 0.004s | Train Loss: 0.012526 |\n",
      "| Epoch: 366/500 | Train Time: 0.004s | Train Loss: 0.012524 |\n",
      "| Epoch: 367/500 | Train Time: 0.004s | Train Loss: 0.012519 |\n",
      "| Epoch: 368/500 | Train Time: 0.004s | Train Loss: 0.012450 |\n",
      "| Epoch: 369/500 | Train Time: 0.004s | Train Loss: 0.012100 |\n",
      "| Epoch: 370/500 | Train Time: 0.004s | Train Loss: 0.012706 |\n",
      "| Epoch: 371/500 | Train Time: 0.004s | Train Loss: 0.012397 |\n",
      "| Epoch: 372/500 | Train Time: 0.004s | Train Loss: 0.012208 |\n",
      "| Epoch: 373/500 | Train Time: 0.004s | Train Loss: 0.012609 |\n",
      "| Epoch: 374/500 | Train Time: 0.004s | Train Loss: 0.012412 |\n",
      "| Epoch: 375/500 | Train Time: 0.004s | Train Loss: 0.012538 |\n",
      "| Epoch: 376/500 | Train Time: 0.004s | Train Loss: 0.012730 |\n",
      "| Epoch: 377/500 | Train Time: 0.004s | Train Loss: 0.012300 |\n",
      "| Epoch: 378/500 | Train Time: 0.004s | Train Loss: 0.012487 |\n",
      "| Epoch: 379/500 | Train Time: 0.004s | Train Loss: 0.012147 |\n",
      "| Epoch: 380/500 | Train Time: 0.004s | Train Loss: 0.012320 |\n",
      "| Epoch: 381/500 | Train Time: 0.004s | Train Loss: 0.012396 |\n",
      "| Epoch: 382/500 | Train Time: 0.004s | Train Loss: 0.012410 |\n",
      "| Epoch: 383/500 | Train Time: 0.004s | Train Loss: 0.011942 |\n",
      "| Epoch: 384/500 | Train Time: 0.004s | Train Loss: 0.012299 |\n",
      "| Epoch: 385/500 | Train Time: 0.004s | Train Loss: 0.012309 |\n",
      "| Epoch: 386/500 | Train Time: 0.004s | Train Loss: 0.012534 |\n",
      "| Epoch: 387/500 | Train Time: 0.004s | Train Loss: 0.012278 |\n",
      "| Epoch: 388/500 | Train Time: 0.004s | Train Loss: 0.012335 |\n",
      "| Epoch: 389/500 | Train Time: 0.004s | Train Loss: 0.012411 |\n",
      "| Epoch: 390/500 | Train Time: 0.004s | Train Loss: 0.012358 |\n",
      "| Epoch: 391/500 | Train Time: 0.004s | Train Loss: 0.012516 |\n",
      "| Epoch: 392/500 | Train Time: 0.004s | Train Loss: 0.012454 |\n",
      "| Epoch: 393/500 | Train Time: 0.004s | Train Loss: 0.012202 |\n",
      "| Epoch: 394/500 | Train Time: 0.004s | Train Loss: 0.012424 |\n",
      "| Epoch: 395/500 | Train Time: 0.004s | Train Loss: 0.012562 |\n",
      "| Epoch: 396/500 | Train Time: 0.004s | Train Loss: 0.012483 |\n",
      "| Epoch: 397/500 | Train Time: 0.004s | Train Loss: 0.012382 |\n",
      "| Epoch: 398/500 | Train Time: 0.004s | Train Loss: 0.012190 |\n",
      "| Epoch: 399/500 | Train Time: 0.004s | Train Loss: 0.012556 |\n",
      "| Epoch: 400/500 | Train Time: 0.004s | Train Loss: 0.012641 |\n",
      "| Epoch: 401/500 | Train Time: 0.004s | Train Loss: 0.012213 |\n",
      "| Epoch: 402/500 | Train Time: 0.004s | Train Loss: 0.012207 |\n",
      "| Epoch: 403/500 | Train Time: 0.004s | Train Loss: 0.012300 |\n",
      "| Epoch: 404/500 | Train Time: 0.004s | Train Loss: 0.012260 |\n",
      "| Epoch: 405/500 | Train Time: 0.004s | Train Loss: 0.012206 |\n",
      "| Epoch: 406/500 | Train Time: 0.004s | Train Loss: 0.012512 |\n",
      "| Epoch: 407/500 | Train Time: 0.004s | Train Loss: 0.012285 |\n",
      "| Epoch: 408/500 | Train Time: 0.004s | Train Loss: 0.012282 |\n",
      "| Epoch: 409/500 | Train Time: 0.005s | Train Loss: 0.012229 |\n",
      "| Epoch: 410/500 | Train Time: 0.004s | Train Loss: 0.012501 |\n",
      "| Epoch: 411/500 | Train Time: 0.004s | Train Loss: 0.012533 |\n",
      "| Epoch: 412/500 | Train Time: 0.004s | Train Loss: 0.012417 |\n",
      "| Epoch: 413/500 | Train Time: 0.004s | Train Loss: 0.012622 |\n",
      "| Epoch: 414/500 | Train Time: 0.004s | Train Loss: 0.012049 |\n",
      "| Epoch: 415/500 | Train Time: 0.004s | Train Loss: 0.012224 |\n",
      "| Epoch: 416/500 | Train Time: 0.004s | Train Loss: 0.012331 |\n",
      "| Epoch: 417/500 | Train Time: 0.004s | Train Loss: 0.012559 |\n",
      "| Epoch: 418/500 | Train Time: 0.004s | Train Loss: 0.012356 |\n",
      "| Epoch: 419/500 | Train Time: 0.004s | Train Loss: 0.011894 |\n",
      "| Epoch: 420/500 | Train Time: 0.004s | Train Loss: 0.012638 |\n",
      "| Epoch: 421/500 | Train Time: 0.004s | Train Loss: 0.012358 |\n",
      "| Epoch: 422/500 | Train Time: 0.004s | Train Loss: 0.012482 |\n",
      "| Epoch: 423/500 | Train Time: 0.004s | Train Loss: 0.012232 |\n",
      "| Epoch: 424/500 | Train Time: 0.004s | Train Loss: 0.012310 |\n",
      "| Epoch: 425/500 | Train Time: 0.004s | Train Loss: 0.012461 |\n",
      "| Epoch: 426/500 | Train Time: 0.004s | Train Loss: 0.012498 |\n",
      "| Epoch: 427/500 | Train Time: 0.004s | Train Loss: 0.012499 |\n",
      "| Epoch: 428/500 | Train Time: 0.004s | Train Loss: 0.012663 |\n",
      "| Epoch: 429/500 | Train Time: 0.004s | Train Loss: 0.012532 |\n",
      "| Epoch: 430/500 | Train Time: 0.004s | Train Loss: 0.012404 |\n",
      "| Epoch: 431/500 | Train Time: 0.004s | Train Loss: 0.012352 |\n",
      "| Epoch: 432/500 | Train Time: 0.004s | Train Loss: 0.012426 |\n",
      "| Epoch: 433/500 | Train Time: 0.004s | Train Loss: 0.012091 |\n",
      "| Epoch: 434/500 | Train Time: 0.004s | Train Loss: 0.012122 |\n",
      "| Epoch: 435/500 | Train Time: 0.003s | Train Loss: 0.012612 |\n",
      "| Epoch: 436/500 | Train Time: 0.004s | Train Loss: 0.012424 |\n",
      "| Epoch: 437/500 | Train Time: 0.004s | Train Loss: 0.012620 |\n",
      "| Epoch: 438/500 | Train Time: 0.004s | Train Loss: 0.012161 |\n",
      "| Epoch: 439/500 | Train Time: 0.004s | Train Loss: 0.012111 |\n",
      "| Epoch: 440/500 | Train Time: 0.004s | Train Loss: 0.011950 |\n",
      "| Epoch: 441/500 | Train Time: 0.004s | Train Loss: 0.012030 |\n",
      "| Epoch: 442/500 | Train Time: 0.004s | Train Loss: 0.012394 |\n",
      "| Epoch: 443/500 | Train Time: 0.004s | Train Loss: 0.011963 |\n",
      "| Epoch: 444/500 | Train Time: 0.004s | Train Loss: 0.012182 |\n",
      "| Epoch: 445/500 | Train Time: 0.004s | Train Loss: 0.012156 |\n",
      "| Epoch: 446/500 | Train Time: 0.004s | Train Loss: 0.012147 |\n",
      "| Epoch: 447/500 | Train Time: 0.004s | Train Loss: 0.012473 |\n",
      "| Epoch: 448/500 | Train Time: 0.004s | Train Loss: 0.012299 |\n",
      "| Epoch: 449/500 | Train Time: 0.004s | Train Loss: 0.012276 |\n",
      "| Epoch: 450/500 | Train Time: 0.004s | Train Loss: 0.011933 |\n",
      "| Epoch: 451/500 | Train Time: 0.004s | Train Loss: 0.012053 |\n",
      "| Epoch: 452/500 | Train Time: 0.004s | Train Loss: 0.011867 |\n",
      "| Epoch: 453/500 | Train Time: 0.004s | Train Loss: 0.012345 |\n",
      "| Epoch: 454/500 | Train Time: 0.004s | Train Loss: 0.012261 |\n",
      "| Epoch: 455/500 | Train Time: 0.004s | Train Loss: 0.012505 |\n",
      "| Epoch: 456/500 | Train Time: 0.004s | Train Loss: 0.012202 |\n",
      "| Epoch: 457/500 | Train Time: 0.004s | Train Loss: 0.012398 |\n",
      "| Epoch: 458/500 | Train Time: 0.004s | Train Loss: 0.012616 |\n",
      "| Epoch: 459/500 | Train Time: 0.004s | Train Loss: 0.012340 |\n",
      "| Epoch: 460/500 | Train Time: 0.004s | Train Loss: 0.012242 |\n",
      "| Epoch: 461/500 | Train Time: 0.004s | Train Loss: 0.012370 |\n",
      "| Epoch: 462/500 | Train Time: 0.004s | Train Loss: 0.011891 |\n",
      "| Epoch: 463/500 | Train Time: 0.005s | Train Loss: 0.011786 |\n",
      "| Epoch: 464/500 | Train Time: 0.004s | Train Loss: 0.012343 |\n",
      "| Epoch: 465/500 | Train Time: 0.004s | Train Loss: 0.012549 |\n",
      "| Epoch: 466/500 | Train Time: 0.004s | Train Loss: 0.012282 |\n",
      "| Epoch: 467/500 | Train Time: 0.004s | Train Loss: 0.012241 |\n",
      "| Epoch: 468/500 | Train Time: 0.004s | Train Loss: 0.012309 |\n",
      "| Epoch: 469/500 | Train Time: 0.004s | Train Loss: 0.012290 |\n",
      "| Epoch: 470/500 | Train Time: 0.004s | Train Loss: 0.012442 |\n",
      "| Epoch: 471/500 | Train Time: 0.004s | Train Loss: 0.012295 |\n",
      "| Epoch: 472/500 | Train Time: 0.004s | Train Loss: 0.012416 |\n",
      "| Epoch: 473/500 | Train Time: 0.004s | Train Loss: 0.012332 |\n",
      "| Epoch: 474/500 | Train Time: 0.004s | Train Loss: 0.012182 |\n",
      "| Epoch: 475/500 | Train Time: 0.004s | Train Loss: 0.012507 |\n",
      "| Epoch: 476/500 | Train Time: 0.004s | Train Loss: 0.012395 |\n",
      "| Epoch: 477/500 | Train Time: 0.004s | Train Loss: 0.012140 |\n",
      "| Epoch: 478/500 | Train Time: 0.004s | Train Loss: 0.012104 |\n",
      "| Epoch: 479/500 | Train Time: 0.004s | Train Loss: 0.012309 |\n",
      "| Epoch: 480/500 | Train Time: 0.004s | Train Loss: 0.012375 |\n",
      "| Epoch: 481/500 | Train Time: 0.004s | Train Loss: 0.012571 |\n",
      "| Epoch: 482/500 | Train Time: 0.004s | Train Loss: 0.012085 |\n",
      "| Epoch: 483/500 | Train Time: 0.004s | Train Loss: 0.012295 |\n",
      "| Epoch: 484/500 | Train Time: 0.004s | Train Loss: 0.012179 |\n",
      "| Epoch: 485/500 | Train Time: 0.004s | Train Loss: 0.011771 |\n",
      "| Epoch: 486/500 | Train Time: 0.004s | Train Loss: 0.012203 |\n",
      "| Epoch: 487/500 | Train Time: 0.004s | Train Loss: 0.012005 |\n",
      "| Epoch: 488/500 | Train Time: 0.004s | Train Loss: 0.012208 |\n",
      "| Epoch: 489/500 | Train Time: 0.004s | Train Loss: 0.012205 |\n",
      "| Epoch: 490/500 | Train Time: 0.004s | Train Loss: 0.012105 |\n",
      "| Epoch: 491/500 | Train Time: 0.004s | Train Loss: 0.012492 |\n",
      "| Epoch: 492/500 | Train Time: 0.004s | Train Loss: 0.012278 |\n",
      "| Epoch: 493/500 | Train Time: 0.004s | Train Loss: 0.012254 |\n",
      "| Epoch: 494/500 | Train Time: 0.004s | Train Loss: 0.012174 |\n",
      "| Epoch: 495/500 | Train Time: 0.004s | Train Loss: 0.012377 |\n",
      "| Epoch: 496/500 | Train Time: 0.004s | Train Loss: 0.012152 |\n",
      "| Epoch: 497/500 | Train Time: 0.004s | Train Loss: 0.011921 |\n",
      "| Epoch: 498/500 | Train Time: 0.004s | Train Loss: 0.012118 |\n",
      "| Epoch: 499/500 | Train Time: 0.004s | Train Loss: 0.012037 |\n",
      "| Epoch: 500/500 | Train Time: 0.004s | Train Loss: 0.012327 |\n",
      "Pretraining Time: 2.188s\n",
      "Finished pretraining.\n",
      "Testing autoencoder...\n",
      "Test Loss: 0.026566\n",
      "Test AUC: 77.13%\n",
      "Test Time: 0.002s\n",
      "Finished testing autoencoder.\n"
     ]
    }
   ],
   "source": [
    "if config[\"pretrain\"]:\n",
    "    # Pretrain model on dataset (via autoencoder)\n",
    "    deepSAD.pretrain(dataset,\n",
    "                     optimizer_name=config['ae_optimizer_name'],\n",
    "                     lr=config['ae_lr'],\n",
    "                     n_epochs=config['ae_n_epochs'],\n",
    "                     lr_milestones=config['ae_lr_milestones'],\n",
    "                     batch_size=config['ae_batch_size'],\n",
    "                     weight_decay=config['ae_weight_decay'],\n",
    "                     device=device,\n",
    "                     n_jobs_dataloader=config['n_jobs_dataloader'])\n",
    "\n",
    "    # Save pretraining results\n",
    "    deepSAD.save_ae_results(export_json=config['export_path'] + '/ae_results.json')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_moment(losses):\n",
    "    _, ax = plt.subplots(figsize=(8, 3), dpi=80)\n",
    "    ax.plot(losses, 'blue', label='train', linewidth=1)\n",
    "    ax.set_title('Loss change in training')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_xlabel('Iteration')\n",
    "    ax.legend(loc='upper right')\n",
    "    # plt.savefig(os.path.join(config['img_dir'], 'loss_dagmm.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x240 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss_moment(deepSAD.ae_loss_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing center c...\n",
      "Center c initialized.\n",
      "Starting training...\n",
      "| Epoch: 001/500 | Train Time: 0.022s | Train Loss: 5.084979 |\n",
      "| Epoch: 002/500 | Train Time: 0.017s | Train Loss: 2.765095 |\n",
      "| Epoch: 003/500 | Train Time: 0.017s | Train Loss: 1.800769 |\n",
      "| Epoch: 004/500 | Train Time: 0.016s | Train Loss: 1.222927 |\n",
      "| Epoch: 005/500 | Train Time: 0.015s | Train Loss: 0.901165 |\n",
      "| Epoch: 006/500 | Train Time: 0.016s | Train Loss: 0.704603 |\n",
      "| Epoch: 007/500 | Train Time: 0.015s | Train Loss: 0.595755 |\n",
      "| Epoch: 008/500 | Train Time: 0.014s | Train Loss: 0.485517 |\n",
      "| Epoch: 009/500 | Train Time: 0.015s | Train Loss: 0.423231 |\n",
      "| Epoch: 010/500 | Train Time: 0.014s | Train Loss: 0.353485 |\n",
      "| Epoch: 011/500 | Train Time: 0.012s | Train Loss: 0.308958 |\n",
      "| Epoch: 012/500 | Train Time: 0.012s | Train Loss: 0.273825 |\n",
      "| Epoch: 013/500 | Train Time: 0.013s | Train Loss: 0.240424 |\n",
      "| Epoch: 014/500 | Train Time: 0.012s | Train Loss: 0.232463 |\n",
      "| Epoch: 015/500 | Train Time: 0.011s | Train Loss: 0.196300 |\n",
      "| Epoch: 016/500 | Train Time: 0.011s | Train Loss: 0.194799 |\n",
      "| Epoch: 017/500 | Train Time: 0.011s | Train Loss: 0.191074 |\n",
      "| Epoch: 018/500 | Train Time: 0.011s | Train Loss: 0.164032 |\n",
      "| Epoch: 019/500 | Train Time: 0.011s | Train Loss: 0.168958 |\n",
      "| Epoch: 020/500 | Train Time: 0.010s | Train Loss: 0.148780 |\n",
      "| Epoch: 021/500 | Train Time: 0.011s | Train Loss: 0.136733 |\n",
      "| Epoch: 022/500 | Train Time: 0.012s | Train Loss: 0.125258 |\n",
      "| Epoch: 023/500 | Train Time: 0.012s | Train Loss: 0.120971 |\n",
      "| Epoch: 024/500 | Train Time: 0.011s | Train Loss: 0.122088 |\n",
      "| Epoch: 025/500 | Train Time: 0.011s | Train Loss: 0.114233 |\n",
      "| Epoch: 026/500 | Train Time: 0.012s | Train Loss: 0.107228 |\n",
      "| Epoch: 027/500 | Train Time: 0.012s | Train Loss: 0.116429 |\n",
      "| Epoch: 028/500 | Train Time: 0.011s | Train Loss: 0.125939 |\n",
      "| Epoch: 029/500 | Train Time: 0.010s | Train Loss: 0.097094 |\n",
      "| Epoch: 030/500 | Train Time: 0.011s | Train Loss: 0.088180 |\n",
      "| Epoch: 031/500 | Train Time: 0.012s | Train Loss: 0.091422 |\n",
      "| Epoch: 032/500 | Train Time: 0.013s | Train Loss: 0.084227 |\n",
      "| Epoch: 033/500 | Train Time: 0.011s | Train Loss: 0.080968 |\n",
      "| Epoch: 034/500 | Train Time: 0.011s | Train Loss: 0.085710 |\n",
      "| Epoch: 035/500 | Train Time: 0.012s | Train Loss: 0.073596 |\n",
      "| Epoch: 036/500 | Train Time: 0.012s | Train Loss: 0.067370 |\n",
      "| Epoch: 037/500 | Train Time: 0.012s | Train Loss: 0.082810 |\n",
      "| Epoch: 038/500 | Train Time: 0.011s | Train Loss: 0.070670 |\n",
      "| Epoch: 039/500 | Train Time: 0.012s | Train Loss: 0.058565 |\n",
      "| Epoch: 040/500 | Train Time: 0.012s | Train Loss: 0.068337 |\n",
      "| Epoch: 041/500 | Train Time: 0.012s | Train Loss: 0.071320 |\n",
      "| Epoch: 042/500 | Train Time: 0.011s | Train Loss: 0.065416 |\n",
      "| Epoch: 043/500 | Train Time: 0.012s | Train Loss: 0.061485 |\n",
      "| Epoch: 044/500 | Train Time: 0.012s | Train Loss: 0.056148 |\n",
      "| Epoch: 045/500 | Train Time: 0.012s | Train Loss: 0.055696 |\n",
      "| Epoch: 046/500 | Train Time: 0.012s | Train Loss: 0.049419 |\n",
      "| Epoch: 047/500 | Train Time: 0.013s | Train Loss: 0.054206 |\n",
      "| Epoch: 048/500 | Train Time: 0.013s | Train Loss: 0.053638 |\n",
      "| Epoch: 049/500 | Train Time: 0.013s | Train Loss: 0.050900 |\n",
      "| Epoch: 050/500 | Train Time: 0.010s | Train Loss: 0.063301 |\n",
      "  LR scheduler: new learning rate is 1e-05\n",
      "| Epoch: 051/500 | Train Time: 0.011s | Train Loss: 0.054654 |\n",
      "| Epoch: 052/500 | Train Time: 0.011s | Train Loss: 0.054306 |\n",
      "| Epoch: 053/500 | Train Time: 0.010s | Train Loss: 0.044575 |\n",
      "| Epoch: 054/500 | Train Time: 0.010s | Train Loss: 0.047540 |\n",
      "| Epoch: 055/500 | Train Time: 0.010s | Train Loss: 0.042940 |\n",
      "| Epoch: 056/500 | Train Time: 0.011s | Train Loss: 0.045290 |\n",
      "| Epoch: 057/500 | Train Time: 0.012s | Train Loss: 0.046190 |\n",
      "| Epoch: 058/500 | Train Time: 0.012s | Train Loss: 0.042392 |\n",
      "| Epoch: 059/500 | Train Time: 0.011s | Train Loss: 0.051453 |\n",
      "| Epoch: 060/500 | Train Time: 0.012s | Train Loss: 0.055262 |\n",
      "| Epoch: 061/500 | Train Time: 0.012s | Train Loss: 0.044466 |\n",
      "| Epoch: 062/500 | Train Time: 0.013s | Train Loss: 0.041621 |\n",
      "| Epoch: 063/500 | Train Time: 0.012s | Train Loss: 0.044538 |\n",
      "| Epoch: 064/500 | Train Time: 0.012s | Train Loss: 0.052958 |\n",
      "| Epoch: 065/500 | Train Time: 0.014s | Train Loss: 0.043527 |\n",
      "| Epoch: 066/500 | Train Time: 0.014s | Train Loss: 0.044743 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sewon/anaconda3/envs/venvs/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:418: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 067/500 | Train Time: 0.014s | Train Loss: 0.045064 |\n",
      "| Epoch: 068/500 | Train Time: 0.012s | Train Loss: 0.048778 |\n",
      "| Epoch: 069/500 | Train Time: 0.012s | Train Loss: 0.045389 |\n",
      "| Epoch: 070/500 | Train Time: 0.012s | Train Loss: 0.047946 |\n",
      "| Epoch: 071/500 | Train Time: 0.011s | Train Loss: 0.046353 |\n",
      "| Epoch: 072/500 | Train Time: 0.012s | Train Loss: 0.046369 |\n",
      "| Epoch: 073/500 | Train Time: 0.012s | Train Loss: 0.047424 |\n",
      "| Epoch: 074/500 | Train Time: 0.013s | Train Loss: 0.042674 |\n",
      "| Epoch: 075/500 | Train Time: 0.012s | Train Loss: 0.041957 |\n",
      "| Epoch: 076/500 | Train Time: 0.012s | Train Loss: 0.048978 |\n",
      "| Epoch: 077/500 | Train Time: 0.013s | Train Loss: 0.040698 |\n",
      "| Epoch: 078/500 | Train Time: 0.012s | Train Loss: 0.042370 |\n",
      "| Epoch: 079/500 | Train Time: 0.011s | Train Loss: 0.045579 |\n",
      "| Epoch: 080/500 | Train Time: 0.011s | Train Loss: 0.041721 |\n",
      "| Epoch: 081/500 | Train Time: 0.012s | Train Loss: 0.045775 |\n",
      "| Epoch: 082/500 | Train Time: 0.012s | Train Loss: 0.041941 |\n",
      "| Epoch: 083/500 | Train Time: 0.013s | Train Loss: 0.041047 |\n",
      "| Epoch: 084/500 | Train Time: 0.013s | Train Loss: 0.036859 |\n",
      "| Epoch: 085/500 | Train Time: 0.013s | Train Loss: 0.044610 |\n",
      "| Epoch: 086/500 | Train Time: 0.013s | Train Loss: 0.041926 |\n",
      "| Epoch: 087/500 | Train Time: 0.013s | Train Loss: 0.039204 |\n",
      "| Epoch: 088/500 | Train Time: 0.013s | Train Loss: 0.040613 |\n",
      "| Epoch: 089/500 | Train Time: 0.013s | Train Loss: 0.054503 |\n",
      "| Epoch: 090/500 | Train Time: 0.013s | Train Loss: 0.039016 |\n",
      "| Epoch: 091/500 | Train Time: 0.012s | Train Loss: 0.042251 |\n",
      "| Epoch: 092/500 | Train Time: 0.012s | Train Loss: 0.041862 |\n",
      "| Epoch: 093/500 | Train Time: 0.012s | Train Loss: 0.041957 |\n",
      "| Epoch: 094/500 | Train Time: 0.012s | Train Loss: 0.043254 |\n",
      "| Epoch: 095/500 | Train Time: 0.012s | Train Loss: 0.038027 |\n",
      "| Epoch: 096/500 | Train Time: 0.013s | Train Loss: 0.040458 |\n",
      "| Epoch: 097/500 | Train Time: 0.014s | Train Loss: 0.037274 |\n",
      "| Epoch: 098/500 | Train Time: 0.013s | Train Loss: 0.050263 |\n",
      "| Epoch: 099/500 | Train Time: 0.013s | Train Loss: 0.042957 |\n",
      "| Epoch: 100/500 | Train Time: 0.017s | Train Loss: 0.037031 |\n",
      "| Epoch: 101/500 | Train Time: 0.013s | Train Loss: 0.037959 |\n",
      "| Epoch: 102/500 | Train Time: 0.014s | Train Loss: 0.051444 |\n",
      "| Epoch: 103/500 | Train Time: 0.013s | Train Loss: 0.041350 |\n",
      "| Epoch: 104/500 | Train Time: 0.014s | Train Loss: 0.052253 |\n",
      "| Epoch: 105/500 | Train Time: 0.014s | Train Loss: 0.044185 |\n",
      "| Epoch: 106/500 | Train Time: 0.015s | Train Loss: 0.044738 |\n",
      "| Epoch: 107/500 | Train Time: 0.015s | Train Loss: 0.045261 |\n",
      "| Epoch: 108/500 | Train Time: 0.016s | Train Loss: 0.042655 |\n",
      "| Epoch: 109/500 | Train Time: 0.016s | Train Loss: 0.039186 |\n",
      "| Epoch: 110/500 | Train Time: 0.016s | Train Loss: 0.043533 |\n",
      "| Epoch: 111/500 | Train Time: 0.017s | Train Loss: 0.034062 |\n",
      "| Epoch: 112/500 | Train Time: 0.017s | Train Loss: 0.035799 |\n",
      "| Epoch: 113/500 | Train Time: 0.017s | Train Loss: 0.036119 |\n",
      "| Epoch: 114/500 | Train Time: 0.017s | Train Loss: 0.033651 |\n",
      "| Epoch: 115/500 | Train Time: 0.016s | Train Loss: 0.040027 |\n",
      "| Epoch: 116/500 | Train Time: 0.015s | Train Loss: 0.035292 |\n",
      "| Epoch: 117/500 | Train Time: 0.016s | Train Loss: 0.038746 |\n",
      "| Epoch: 118/500 | Train Time: 0.017s | Train Loss: 0.038605 |\n",
      "| Epoch: 119/500 | Train Time: 0.015s | Train Loss: 0.038787 |\n",
      "| Epoch: 120/500 | Train Time: 0.015s | Train Loss: 0.034953 |\n",
      "| Epoch: 121/500 | Train Time: 0.015s | Train Loss: 0.036483 |\n",
      "| Epoch: 122/500 | Train Time: 0.015s | Train Loss: 0.034047 |\n",
      "| Epoch: 123/500 | Train Time: 0.015s | Train Loss: 0.037780 |\n",
      "| Epoch: 124/500 | Train Time: 0.015s | Train Loss: 0.053651 |\n",
      "| Epoch: 125/500 | Train Time: 0.015s | Train Loss: 0.036973 |\n",
      "| Epoch: 126/500 | Train Time: 0.014s | Train Loss: 0.041244 |\n",
      "| Epoch: 127/500 | Train Time: 0.014s | Train Loss: 0.036569 |\n",
      "| Epoch: 128/500 | Train Time: 0.014s | Train Loss: 0.042992 |\n",
      "| Epoch: 129/500 | Train Time: 0.014s | Train Loss: 0.034183 |\n",
      "| Epoch: 130/500 | Train Time: 0.014s | Train Loss: 0.041214 |\n",
      "| Epoch: 131/500 | Train Time: 0.014s | Train Loss: 0.036246 |\n",
      "| Epoch: 132/500 | Train Time: 0.015s | Train Loss: 0.047904 |\n",
      "| Epoch: 133/500 | Train Time: 0.013s | Train Loss: 0.039433 |\n",
      "| Epoch: 134/500 | Train Time: 0.014s | Train Loss: 0.038959 |\n",
      "| Epoch: 135/500 | Train Time: 0.014s | Train Loss: 0.038051 |\n",
      "| Epoch: 136/500 | Train Time: 0.014s | Train Loss: 0.040170 |\n",
      "| Epoch: 137/500 | Train Time: 0.013s | Train Loss: 0.031937 |\n",
      "| Epoch: 138/500 | Train Time: 0.014s | Train Loss: 0.032421 |\n",
      "| Epoch: 139/500 | Train Time: 0.015s | Train Loss: 0.029039 |\n",
      "| Epoch: 140/500 | Train Time: 0.013s | Train Loss: 0.037179 |\n",
      "| Epoch: 141/500 | Train Time: 0.014s | Train Loss: 0.035258 |\n",
      "| Epoch: 142/500 | Train Time: 0.014s | Train Loss: 0.039315 |\n",
      "| Epoch: 143/500 | Train Time: 0.014s | Train Loss: 0.049821 |\n",
      "| Epoch: 144/500 | Train Time: 0.014s | Train Loss: 0.035276 |\n",
      "| Epoch: 145/500 | Train Time: 0.014s | Train Loss: 0.036792 |\n",
      "| Epoch: 146/500 | Train Time: 0.014s | Train Loss: 0.035493 |\n",
      "| Epoch: 147/500 | Train Time: 0.013s | Train Loss: 0.036255 |\n",
      "| Epoch: 148/500 | Train Time: 0.013s | Train Loss: 0.034122 |\n",
      "| Epoch: 149/500 | Train Time: 0.013s | Train Loss: 0.033083 |\n",
      "| Epoch: 150/500 | Train Time: 0.013s | Train Loss: 0.037152 |\n",
      "| Epoch: 151/500 | Train Time: 0.012s | Train Loss: 0.039539 |\n",
      "| Epoch: 152/500 | Train Time: 0.014s | Train Loss: 0.034990 |\n",
      "| Epoch: 153/500 | Train Time: 0.014s | Train Loss: 0.039123 |\n",
      "| Epoch: 154/500 | Train Time: 0.013s | Train Loss: 0.032982 |\n",
      "| Epoch: 155/500 | Train Time: 0.013s | Train Loss: 0.034678 |\n",
      "| Epoch: 156/500 | Train Time: 0.014s | Train Loss: 0.032044 |\n",
      "| Epoch: 157/500 | Train Time: 0.013s | Train Loss: 0.036761 |\n",
      "| Epoch: 158/500 | Train Time: 0.014s | Train Loss: 0.030772 |\n",
      "| Epoch: 159/500 | Train Time: 0.012s | Train Loss: 0.040928 |\n",
      "| Epoch: 160/500 | Train Time: 0.012s | Train Loss: 0.037219 |\n",
      "| Epoch: 161/500 | Train Time: 0.012s | Train Loss: 0.045354 |\n",
      "| Epoch: 162/500 | Train Time: 0.012s | Train Loss: 0.034007 |\n",
      "| Epoch: 163/500 | Train Time: 0.011s | Train Loss: 0.035042 |\n",
      "| Epoch: 164/500 | Train Time: 0.012s | Train Loss: 0.030914 |\n",
      "| Epoch: 165/500 | Train Time: 0.012s | Train Loss: 0.032023 |\n",
      "| Epoch: 166/500 | Train Time: 0.012s | Train Loss: 0.034860 |\n",
      "| Epoch: 167/500 | Train Time: 0.011s | Train Loss: 0.030978 |\n",
      "| Epoch: 168/500 | Train Time: 0.011s | Train Loss: 0.032630 |\n",
      "| Epoch: 169/500 | Train Time: 0.011s | Train Loss: 0.036155 |\n",
      "| Epoch: 170/500 | Train Time: 0.012s | Train Loss: 0.032051 |\n",
      "| Epoch: 171/500 | Train Time: 0.012s | Train Loss: 0.030685 |\n",
      "| Epoch: 172/500 | Train Time: 0.011s | Train Loss: 0.037383 |\n",
      "| Epoch: 173/500 | Train Time: 0.012s | Train Loss: 0.033595 |\n",
      "| Epoch: 174/500 | Train Time: 0.012s | Train Loss: 0.040325 |\n",
      "| Epoch: 175/500 | Train Time: 0.012s | Train Loss: 0.035892 |\n",
      "| Epoch: 176/500 | Train Time: 0.011s | Train Loss: 0.033046 |\n",
      "| Epoch: 177/500 | Train Time: 0.011s | Train Loss: 0.029544 |\n",
      "| Epoch: 178/500 | Train Time: 0.011s | Train Loss: 0.032487 |\n",
      "| Epoch: 179/500 | Train Time: 0.012s | Train Loss: 0.032451 |\n",
      "| Epoch: 180/500 | Train Time: 0.010s | Train Loss: 0.032426 |\n",
      "| Epoch: 181/500 | Train Time: 0.010s | Train Loss: 0.031368 |\n",
      "| Epoch: 182/500 | Train Time: 0.011s | Train Loss: 0.030952 |\n",
      "| Epoch: 183/500 | Train Time: 0.011s | Train Loss: 0.027790 |\n",
      "| Epoch: 184/500 | Train Time: 0.012s | Train Loss: 0.026826 |\n",
      "| Epoch: 185/500 | Train Time: 0.011s | Train Loss: 0.033057 |\n",
      "| Epoch: 186/500 | Train Time: 0.012s | Train Loss: 0.029363 |\n",
      "| Epoch: 187/500 | Train Time: 0.012s | Train Loss: 0.032028 |\n",
      "| Epoch: 188/500 | Train Time: 0.012s | Train Loss: 0.032624 |\n",
      "| Epoch: 189/500 | Train Time: 0.011s | Train Loss: 0.031912 |\n",
      "| Epoch: 190/500 | Train Time: 0.011s | Train Loss: 0.027755 |\n",
      "| Epoch: 191/500 | Train Time: 0.011s | Train Loss: 0.035547 |\n",
      "| Epoch: 192/500 | Train Time: 0.012s | Train Loss: 0.028740 |\n",
      "| Epoch: 193/500 | Train Time: 0.012s | Train Loss: 0.031682 |\n",
      "| Epoch: 194/500 | Train Time: 0.012s | Train Loss: 0.033821 |\n",
      "| Epoch: 195/500 | Train Time: 0.011s | Train Loss: 0.030071 |\n",
      "| Epoch: 196/500 | Train Time: 0.011s | Train Loss: 0.031834 |\n",
      "| Epoch: 197/500 | Train Time: 0.011s | Train Loss: 0.033340 |\n",
      "| Epoch: 198/500 | Train Time: 0.011s | Train Loss: 0.029829 |\n",
      "| Epoch: 199/500 | Train Time: 0.010s | Train Loss: 0.034705 |\n",
      "| Epoch: 200/500 | Train Time: 0.010s | Train Loss: 0.029790 |\n",
      "| Epoch: 201/500 | Train Time: 0.011s | Train Loss: 0.049562 |\n",
      "| Epoch: 202/500 | Train Time: 0.012s | Train Loss: 0.031907 |\n",
      "| Epoch: 203/500 | Train Time: 0.010s | Train Loss: 0.031390 |\n",
      "| Epoch: 204/500 | Train Time: 0.011s | Train Loss: 0.035021 |\n",
      "| Epoch: 205/500 | Train Time: 0.011s | Train Loss: 0.028429 |\n",
      "| Epoch: 206/500 | Train Time: 0.012s | Train Loss: 0.029852 |\n",
      "| Epoch: 207/500 | Train Time: 0.011s | Train Loss: 0.035518 |\n",
      "| Epoch: 208/500 | Train Time: 0.011s | Train Loss: 0.032050 |\n",
      "| Epoch: 209/500 | Train Time: 0.012s | Train Loss: 0.029404 |\n",
      "| Epoch: 210/500 | Train Time: 0.011s | Train Loss: 0.033726 |\n",
      "| Epoch: 211/500 | Train Time: 0.012s | Train Loss: 0.029995 |\n",
      "| Epoch: 212/500 | Train Time: 0.010s | Train Loss: 0.036951 |\n",
      "| Epoch: 213/500 | Train Time: 0.012s | Train Loss: 0.035266 |\n",
      "| Epoch: 214/500 | Train Time: 0.011s | Train Loss: 0.041524 |\n",
      "| Epoch: 215/500 | Train Time: 0.011s | Train Loss: 0.027554 |\n",
      "| Epoch: 216/500 | Train Time: 0.011s | Train Loss: 0.033944 |\n",
      "| Epoch: 217/500 | Train Time: 0.011s | Train Loss: 0.027406 |\n",
      "| Epoch: 218/500 | Train Time: 0.012s | Train Loss: 0.029456 |\n",
      "| Epoch: 219/500 | Train Time: 0.012s | Train Loss: 0.035367 |\n",
      "| Epoch: 220/500 | Train Time: 0.013s | Train Loss: 0.029883 |\n",
      "| Epoch: 221/500 | Train Time: 0.011s | Train Loss: 0.029156 |\n",
      "| Epoch: 222/500 | Train Time: 0.013s | Train Loss: 0.024811 |\n",
      "| Epoch: 223/500 | Train Time: 0.012s | Train Loss: 0.029920 |\n",
      "| Epoch: 224/500 | Train Time: 0.012s | Train Loss: 0.029148 |\n",
      "| Epoch: 225/500 | Train Time: 0.012s | Train Loss: 0.030046 |\n",
      "| Epoch: 226/500 | Train Time: 0.013s | Train Loss: 0.029256 |\n",
      "| Epoch: 227/500 | Train Time: 0.014s | Train Loss: 0.030343 |\n",
      "| Epoch: 228/500 | Train Time: 0.014s | Train Loss: 0.024205 |\n",
      "| Epoch: 229/500 | Train Time: 0.013s | Train Loss: 0.033501 |\n",
      "| Epoch: 230/500 | Train Time: 0.014s | Train Loss: 0.026033 |\n",
      "| Epoch: 231/500 | Train Time: 0.013s | Train Loss: 0.026182 |\n",
      "| Epoch: 232/500 | Train Time: 0.012s | Train Loss: 0.025408 |\n",
      "| Epoch: 233/500 | Train Time: 0.013s | Train Loss: 0.031185 |\n",
      "| Epoch: 234/500 | Train Time: 0.013s | Train Loss: 0.025800 |\n",
      "| Epoch: 235/500 | Train Time: 0.013s | Train Loss: 0.032526 |\n",
      "| Epoch: 236/500 | Train Time: 0.013s | Train Loss: 0.025418 |\n",
      "| Epoch: 237/500 | Train Time: 0.012s | Train Loss: 0.031223 |\n",
      "| Epoch: 238/500 | Train Time: 0.013s | Train Loss: 0.039340 |\n",
      "| Epoch: 239/500 | Train Time: 0.013s | Train Loss: 0.025949 |\n",
      "| Epoch: 240/500 | Train Time: 0.013s | Train Loss: 0.032412 |\n",
      "| Epoch: 241/500 | Train Time: 0.013s | Train Loss: 0.030754 |\n",
      "| Epoch: 242/500 | Train Time: 0.012s | Train Loss: 0.031384 |\n",
      "| Epoch: 243/500 | Train Time: 0.013s | Train Loss: 0.026335 |\n",
      "| Epoch: 244/500 | Train Time: 0.013s | Train Loss: 0.033731 |\n",
      "| Epoch: 245/500 | Train Time: 0.013s | Train Loss: 0.029019 |\n",
      "| Epoch: 246/500 | Train Time: 0.013s | Train Loss: 0.031454 |\n",
      "| Epoch: 247/500 | Train Time: 0.012s | Train Loss: 0.024072 |\n",
      "| Epoch: 248/500 | Train Time: 0.011s | Train Loss: 0.028850 |\n",
      "| Epoch: 249/500 | Train Time: 0.012s | Train Loss: 0.032946 |\n",
      "| Epoch: 250/500 | Train Time: 0.012s | Train Loss: 0.029483 |\n",
      "  LR scheduler: new learning rate is 1e-06\n",
      "| Epoch: 251/500 | Train Time: 0.013s | Train Loss: 0.023797 |\n",
      "| Epoch: 252/500 | Train Time: 0.012s | Train Loss: 0.027752 |\n",
      "| Epoch: 253/500 | Train Time: 0.012s | Train Loss: 0.027890 |\n",
      "| Epoch: 254/500 | Train Time: 0.012s | Train Loss: 0.024880 |\n",
      "| Epoch: 255/500 | Train Time: 0.013s | Train Loss: 0.030515 |\n",
      "| Epoch: 256/500 | Train Time: 0.012s | Train Loss: 0.029071 |\n",
      "| Epoch: 257/500 | Train Time: 0.012s | Train Loss: 0.026498 |\n",
      "| Epoch: 258/500 | Train Time: 0.018s | Train Loss: 0.022208 |\n",
      "| Epoch: 259/500 | Train Time: 0.013s | Train Loss: 0.024504 |\n",
      "| Epoch: 260/500 | Train Time: 0.012s | Train Loss: 0.020678 |\n",
      "| Epoch: 261/500 | Train Time: 0.011s | Train Loss: 0.023397 |\n",
      "| Epoch: 262/500 | Train Time: 0.012s | Train Loss: 0.029837 |\n",
      "| Epoch: 263/500 | Train Time: 0.013s | Train Loss: 0.026394 |\n",
      "| Epoch: 264/500 | Train Time: 0.012s | Train Loss: 0.026073 |\n",
      "| Epoch: 265/500 | Train Time: 0.011s | Train Loss: 0.025790 |\n",
      "| Epoch: 266/500 | Train Time: 0.012s | Train Loss: 0.023545 |\n",
      "| Epoch: 267/500 | Train Time: 0.012s | Train Loss: 0.023148 |\n",
      "| Epoch: 268/500 | Train Time: 0.012s | Train Loss: 0.027615 |\n",
      "| Epoch: 269/500 | Train Time: 0.011s | Train Loss: 0.024231 |\n",
      "| Epoch: 270/500 | Train Time: 0.012s | Train Loss: 0.023997 |\n",
      "| Epoch: 271/500 | Train Time: 0.011s | Train Loss: 0.043362 |\n",
      "| Epoch: 272/500 | Train Time: 0.011s | Train Loss: 0.031177 |\n",
      "| Epoch: 273/500 | Train Time: 0.010s | Train Loss: 0.022233 |\n",
      "| Epoch: 274/500 | Train Time: 0.011s | Train Loss: 0.028698 |\n",
      "| Epoch: 275/500 | Train Time: 0.011s | Train Loss: 0.021565 |\n",
      "| Epoch: 276/500 | Train Time: 0.011s | Train Loss: 0.026674 |\n",
      "| Epoch: 277/500 | Train Time: 0.011s | Train Loss: 0.030685 |\n",
      "| Epoch: 278/500 | Train Time: 0.011s | Train Loss: 0.031065 |\n",
      "| Epoch: 279/500 | Train Time: 0.012s | Train Loss: 0.023682 |\n",
      "| Epoch: 280/500 | Train Time: 0.011s | Train Loss: 0.026703 |\n",
      "| Epoch: 281/500 | Train Time: 0.012s | Train Loss: 0.026312 |\n",
      "| Epoch: 282/500 | Train Time: 0.011s | Train Loss: 0.031403 |\n",
      "| Epoch: 283/500 | Train Time: 0.010s | Train Loss: 0.020422 |\n",
      "| Epoch: 284/500 | Train Time: 0.011s | Train Loss: 0.028001 |\n",
      "| Epoch: 285/500 | Train Time: 0.011s | Train Loss: 0.025950 |\n",
      "| Epoch: 286/500 | Train Time: 0.011s | Train Loss: 0.029236 |\n",
      "| Epoch: 287/500 | Train Time: 0.010s | Train Loss: 0.022966 |\n",
      "| Epoch: 288/500 | Train Time: 0.010s | Train Loss: 0.021518 |\n",
      "| Epoch: 289/500 | Train Time: 0.009s | Train Loss: 0.026496 |\n",
      "| Epoch: 290/500 | Train Time: 0.010s | Train Loss: 0.025225 |\n",
      "| Epoch: 291/500 | Train Time: 0.010s | Train Loss: 0.024617 |\n",
      "| Epoch: 292/500 | Train Time: 0.009s | Train Loss: 0.020144 |\n",
      "| Epoch: 293/500 | Train Time: 0.009s | Train Loss: 0.027376 |\n",
      "| Epoch: 294/500 | Train Time: 0.009s | Train Loss: 0.026411 |\n",
      "| Epoch: 295/500 | Train Time: 0.010s | Train Loss: 0.021498 |\n",
      "| Epoch: 296/500 | Train Time: 0.010s | Train Loss: 0.029115 |\n",
      "| Epoch: 297/500 | Train Time: 0.010s | Train Loss: 0.026082 |\n",
      "| Epoch: 298/500 | Train Time: 0.009s | Train Loss: 0.024109 |\n",
      "| Epoch: 299/500 | Train Time: 0.009s | Train Loss: 0.024404 |\n",
      "| Epoch: 300/500 | Train Time: 0.009s | Train Loss: 0.025014 |\n",
      "| Epoch: 301/500 | Train Time: 0.010s | Train Loss: 0.023831 |\n",
      "| Epoch: 302/500 | Train Time: 0.010s | Train Loss: 0.026628 |\n",
      "| Epoch: 303/500 | Train Time: 0.011s | Train Loss: 0.021084 |\n",
      "| Epoch: 304/500 | Train Time: 0.010s | Train Loss: 0.030715 |\n",
      "| Epoch: 305/500 | Train Time: 0.010s | Train Loss: 0.025166 |\n",
      "| Epoch: 306/500 | Train Time: 0.010s | Train Loss: 0.024704 |\n",
      "| Epoch: 307/500 | Train Time: 0.011s | Train Loss: 0.024865 |\n",
      "| Epoch: 308/500 | Train Time: 0.010s | Train Loss: 0.024775 |\n",
      "| Epoch: 309/500 | Train Time: 0.010s | Train Loss: 0.023975 |\n",
      "| Epoch: 310/500 | Train Time: 0.010s | Train Loss: 0.024058 |\n",
      "| Epoch: 311/500 | Train Time: 0.011s | Train Loss: 0.032034 |\n",
      "| Epoch: 312/500 | Train Time: 0.011s | Train Loss: 0.028074 |\n",
      "| Epoch: 313/500 | Train Time: 0.010s | Train Loss: 0.026388 |\n",
      "| Epoch: 314/500 | Train Time: 0.010s | Train Loss: 0.027605 |\n",
      "| Epoch: 315/500 | Train Time: 0.010s | Train Loss: 0.024155 |\n",
      "| Epoch: 316/500 | Train Time: 0.010s | Train Loss: 0.024749 |\n",
      "| Epoch: 317/500 | Train Time: 0.010s | Train Loss: 0.023411 |\n",
      "| Epoch: 318/500 | Train Time: 0.009s | Train Loss: 0.020894 |\n",
      "| Epoch: 319/500 | Train Time: 0.009s | Train Loss: 0.022646 |\n",
      "| Epoch: 320/500 | Train Time: 0.009s | Train Loss: 0.026079 |\n",
      "| Epoch: 321/500 | Train Time: 0.009s | Train Loss: 0.024355 |\n",
      "| Epoch: 322/500 | Train Time: 0.010s | Train Loss: 0.031006 |\n",
      "| Epoch: 323/500 | Train Time: 0.011s | Train Loss: 0.026466 |\n",
      "| Epoch: 324/500 | Train Time: 0.010s | Train Loss: 0.022975 |\n",
      "| Epoch: 325/500 | Train Time: 0.009s | Train Loss: 0.024989 |\n",
      "| Epoch: 326/500 | Train Time: 0.008s | Train Loss: 0.020921 |\n",
      "| Epoch: 327/500 | Train Time: 0.008s | Train Loss: 0.024643 |\n",
      "| Epoch: 328/500 | Train Time: 0.008s | Train Loss: 0.023615 |\n",
      "| Epoch: 329/500 | Train Time: 0.008s | Train Loss: 0.026086 |\n",
      "| Epoch: 330/500 | Train Time: 0.008s | Train Loss: 0.023196 |\n",
      "| Epoch: 331/500 | Train Time: 0.008s | Train Loss: 0.024625 |\n",
      "| Epoch: 332/500 | Train Time: 0.008s | Train Loss: 0.031250 |\n",
      "| Epoch: 333/500 | Train Time: 0.008s | Train Loss: 0.027616 |\n",
      "| Epoch: 334/500 | Train Time: 0.008s | Train Loss: 0.025808 |\n",
      "| Epoch: 335/500 | Train Time: 0.008s | Train Loss: 0.026376 |\n",
      "| Epoch: 336/500 | Train Time: 0.008s | Train Loss: 0.027157 |\n",
      "| Epoch: 337/500 | Train Time: 0.008s | Train Loss: 0.031739 |\n",
      "| Epoch: 338/500 | Train Time: 0.007s | Train Loss: 0.024191 |\n",
      "| Epoch: 339/500 | Train Time: 0.008s | Train Loss: 0.026831 |\n",
      "| Epoch: 340/500 | Train Time: 0.008s | Train Loss: 0.025857 |\n",
      "| Epoch: 341/500 | Train Time: 0.008s | Train Loss: 0.031198 |\n",
      "| Epoch: 342/500 | Train Time: 0.008s | Train Loss: 0.023787 |\n",
      "| Epoch: 343/500 | Train Time: 0.008s | Train Loss: 0.023767 |\n",
      "| Epoch: 344/500 | Train Time: 0.011s | Train Loss: 0.021541 |\n",
      "| Epoch: 345/500 | Train Time: 0.012s | Train Loss: 0.022908 |\n",
      "| Epoch: 346/500 | Train Time: 0.013s | Train Loss: 0.025185 |\n",
      "| Epoch: 347/500 | Train Time: 0.014s | Train Loss: 0.025140 |\n",
      "| Epoch: 348/500 | Train Time: 0.015s | Train Loss: 0.024785 |\n",
      "| Epoch: 349/500 | Train Time: 0.016s | Train Loss: 0.034585 |\n",
      "| Epoch: 350/500 | Train Time: 0.015s | Train Loss: 0.028636 |\n",
      "| Epoch: 351/500 | Train Time: 0.013s | Train Loss: 0.031733 |\n",
      "| Epoch: 352/500 | Train Time: 0.012s | Train Loss: 0.022516 |\n",
      "| Epoch: 353/500 | Train Time: 0.010s | Train Loss: 0.027221 |\n",
      "| Epoch: 354/500 | Train Time: 0.009s | Train Loss: 0.023614 |\n",
      "| Epoch: 355/500 | Train Time: 0.009s | Train Loss: 0.026648 |\n",
      "| Epoch: 356/500 | Train Time: 0.009s | Train Loss: 0.028205 |\n",
      "| Epoch: 357/500 | Train Time: 0.009s | Train Loss: 0.027042 |\n",
      "| Epoch: 358/500 | Train Time: 0.009s | Train Loss: 0.024341 |\n",
      "| Epoch: 359/500 | Train Time: 0.009s | Train Loss: 0.025705 |\n",
      "| Epoch: 360/500 | Train Time: 0.009s | Train Loss: 0.025142 |\n",
      "| Epoch: 361/500 | Train Time: 0.009s | Train Loss: 0.034726 |\n",
      "| Epoch: 362/500 | Train Time: 0.009s | Train Loss: 0.029183 |\n",
      "| Epoch: 363/500 | Train Time: 0.009s | Train Loss: 0.026881 |\n",
      "| Epoch: 364/500 | Train Time: 0.009s | Train Loss: 0.028881 |\n",
      "| Epoch: 365/500 | Train Time: 0.009s | Train Loss: 0.028267 |\n",
      "| Epoch: 366/500 | Train Time: 0.010s | Train Loss: 0.022515 |\n",
      "| Epoch: 367/500 | Train Time: 0.010s | Train Loss: 0.028156 |\n",
      "| Epoch: 368/500 | Train Time: 0.009s | Train Loss: 0.025196 |\n",
      "| Epoch: 369/500 | Train Time: 0.010s | Train Loss: 0.029280 |\n",
      "| Epoch: 370/500 | Train Time: 0.010s | Train Loss: 0.024683 |\n",
      "| Epoch: 371/500 | Train Time: 0.010s | Train Loss: 0.023824 |\n",
      "| Epoch: 372/500 | Train Time: 0.010s | Train Loss: 0.024075 |\n",
      "| Epoch: 373/500 | Train Time: 0.010s | Train Loss: 0.028698 |\n",
      "| Epoch: 374/500 | Train Time: 0.010s | Train Loss: 0.026719 |\n",
      "| Epoch: 375/500 | Train Time: 0.010s | Train Loss: 0.024641 |\n",
      "| Epoch: 376/500 | Train Time: 0.010s | Train Loss: 0.025928 |\n",
      "| Epoch: 377/500 | Train Time: 0.011s | Train Loss: 0.021417 |\n",
      "| Epoch: 378/500 | Train Time: 0.011s | Train Loss: 0.027005 |\n",
      "| Epoch: 379/500 | Train Time: 0.010s | Train Loss: 0.024464 |\n",
      "| Epoch: 380/500 | Train Time: 0.010s | Train Loss: 0.022049 |\n",
      "| Epoch: 381/500 | Train Time: 0.011s | Train Loss: 0.022723 |\n",
      "| Epoch: 382/500 | Train Time: 0.009s | Train Loss: 0.023449 |\n",
      "| Epoch: 383/500 | Train Time: 0.009s | Train Loss: 0.027096 |\n",
      "| Epoch: 384/500 | Train Time: 0.008s | Train Loss: 0.026723 |\n",
      "| Epoch: 385/500 | Train Time: 0.009s | Train Loss: 0.025040 |\n",
      "| Epoch: 386/500 | Train Time: 0.009s | Train Loss: 0.021584 |\n",
      "| Epoch: 387/500 | Train Time: 0.009s | Train Loss: 0.034577 |\n",
      "| Epoch: 388/500 | Train Time: 0.008s | Train Loss: 0.024789 |\n",
      "| Epoch: 389/500 | Train Time: 0.008s | Train Loss: 0.024953 |\n",
      "| Epoch: 390/500 | Train Time: 0.008s | Train Loss: 0.024734 |\n",
      "| Epoch: 391/500 | Train Time: 0.009s | Train Loss: 0.024827 |\n",
      "| Epoch: 392/500 | Train Time: 0.008s | Train Loss: 0.022433 |\n",
      "| Epoch: 393/500 | Train Time: 0.008s | Train Loss: 0.020973 |\n",
      "| Epoch: 394/500 | Train Time: 0.008s | Train Loss: 0.025998 |\n",
      "| Epoch: 395/500 | Train Time: 0.008s | Train Loss: 0.023867 |\n",
      "| Epoch: 396/500 | Train Time: 0.008s | Train Loss: 0.024394 |\n",
      "| Epoch: 397/500 | Train Time: 0.008s | Train Loss: 0.023316 |\n",
      "| Epoch: 398/500 | Train Time: 0.007s | Train Loss: 0.028810 |\n",
      "| Epoch: 399/500 | Train Time: 0.007s | Train Loss: 0.026042 |\n",
      "| Epoch: 400/500 | Train Time: 0.008s | Train Loss: 0.022098 |\n",
      "| Epoch: 401/500 | Train Time: 0.008s | Train Loss: 0.021475 |\n",
      "| Epoch: 402/500 | Train Time: 0.008s | Train Loss: 0.022250 |\n",
      "| Epoch: 403/500 | Train Time: 0.008s | Train Loss: 0.023915 |\n",
      "| Epoch: 404/500 | Train Time: 0.008s | Train Loss: 0.023812 |\n",
      "| Epoch: 405/500 | Train Time: 0.008s | Train Loss: 0.025187 |\n",
      "| Epoch: 406/500 | Train Time: 0.008s | Train Loss: 0.025564 |\n",
      "| Epoch: 407/500 | Train Time: 0.008s | Train Loss: 0.023352 |\n",
      "| Epoch: 408/500 | Train Time: 0.008s | Train Loss: 0.021553 |\n",
      "| Epoch: 409/500 | Train Time: 0.008s | Train Loss: 0.022115 |\n",
      "| Epoch: 410/500 | Train Time: 0.008s | Train Loss: 0.025691 |\n",
      "| Epoch: 411/500 | Train Time: 0.008s | Train Loss: 0.026555 |\n",
      "| Epoch: 412/500 | Train Time: 0.009s | Train Loss: 0.024546 |\n",
      "| Epoch: 413/500 | Train Time: 0.008s | Train Loss: 0.027613 |\n",
      "| Epoch: 414/500 | Train Time: 0.007s | Train Loss: 0.021720 |\n",
      "| Epoch: 415/500 | Train Time: 0.008s | Train Loss: 0.026007 |\n",
      "| Epoch: 416/500 | Train Time: 0.008s | Train Loss: 0.022518 |\n",
      "| Epoch: 417/500 | Train Time: 0.008s | Train Loss: 0.026290 |\n",
      "| Epoch: 418/500 | Train Time: 0.008s | Train Loss: 0.023642 |\n",
      "| Epoch: 419/500 | Train Time: 0.008s | Train Loss: 0.021096 |\n",
      "| Epoch: 420/500 | Train Time: 0.008s | Train Loss: 0.022719 |\n",
      "| Epoch: 421/500 | Train Time: 0.008s | Train Loss: 0.026025 |\n",
      "| Epoch: 422/500 | Train Time: 0.008s | Train Loss: 0.027949 |\n",
      "| Epoch: 423/500 | Train Time: 0.008s | Train Loss: 0.027105 |\n",
      "| Epoch: 424/500 | Train Time: 0.008s | Train Loss: 0.023343 |\n",
      "| Epoch: 425/500 | Train Time: 0.008s | Train Loss: 0.024656 |\n",
      "| Epoch: 426/500 | Train Time: 0.008s | Train Loss: 0.021303 |\n",
      "| Epoch: 427/500 | Train Time: 0.008s | Train Loss: 0.022627 |\n",
      "| Epoch: 428/500 | Train Time: 0.008s | Train Loss: 0.021392 |\n",
      "| Epoch: 429/500 | Train Time: 0.008s | Train Loss: 0.023398 |\n",
      "| Epoch: 430/500 | Train Time: 0.008s | Train Loss: 0.031898 |\n",
      "| Epoch: 431/500 | Train Time: 0.008s | Train Loss: 0.025303 |\n",
      "| Epoch: 432/500 | Train Time: 0.008s | Train Loss: 0.022238 |\n",
      "| Epoch: 433/500 | Train Time: 0.008s | Train Loss: 0.026085 |\n",
      "| Epoch: 434/500 | Train Time: 0.007s | Train Loss: 0.023264 |\n",
      "| Epoch: 435/500 | Train Time: 0.007s | Train Loss: 0.028807 |\n",
      "| Epoch: 436/500 | Train Time: 0.008s | Train Loss: 0.028266 |\n",
      "| Epoch: 437/500 | Train Time: 0.008s | Train Loss: 0.021064 |\n",
      "| Epoch: 438/500 | Train Time: 0.008s | Train Loss: 0.025665 |\n",
      "| Epoch: 439/500 | Train Time: 0.007s | Train Loss: 0.021377 |\n",
      "| Epoch: 440/500 | Train Time: 0.007s | Train Loss: 0.020457 |\n",
      "| Epoch: 441/500 | Train Time: 0.007s | Train Loss: 0.023257 |\n",
      "| Epoch: 442/500 | Train Time: 0.008s | Train Loss: 0.024481 |\n",
      "| Epoch: 443/500 | Train Time: 0.009s | Train Loss: 0.021443 |\n",
      "| Epoch: 444/500 | Train Time: 0.008s | Train Loss: 0.025693 |\n",
      "| Epoch: 445/500 | Train Time: 0.008s | Train Loss: 0.029700 |\n",
      "| Epoch: 446/500 | Train Time: 0.008s | Train Loss: 0.023889 |\n",
      "| Epoch: 447/500 | Train Time: 0.007s | Train Loss: 0.028235 |\n",
      "| Epoch: 448/500 | Train Time: 0.008s | Train Loss: 0.023735 |\n",
      "| Epoch: 449/500 | Train Time: 0.009s | Train Loss: 0.030862 |\n",
      "| Epoch: 450/500 | Train Time: 0.008s | Train Loss: 0.021525 |\n",
      "| Epoch: 451/500 | Train Time: 0.008s | Train Loss: 0.026108 |\n",
      "| Epoch: 452/500 | Train Time: 0.008s | Train Loss: 0.026572 |\n",
      "| Epoch: 453/500 | Train Time: 0.007s | Train Loss: 0.021518 |\n",
      "| Epoch: 454/500 | Train Time: 0.008s | Train Loss: 0.021717 |\n",
      "| Epoch: 455/500 | Train Time: 0.008s | Train Loss: 0.021102 |\n",
      "| Epoch: 456/500 | Train Time: 0.008s | Train Loss: 0.025656 |\n",
      "| Epoch: 457/500 | Train Time: 0.008s | Train Loss: 0.027514 |\n",
      "| Epoch: 458/500 | Train Time: 0.008s | Train Loss: 0.021153 |\n",
      "| Epoch: 459/500 | Train Time: 0.008s | Train Loss: 0.019949 |\n",
      "| Epoch: 460/500 | Train Time: 0.007s | Train Loss: 0.039848 |\n",
      "| Epoch: 461/500 | Train Time: 0.008s | Train Loss: 0.025690 |\n",
      "| Epoch: 462/500 | Train Time: 0.008s | Train Loss: 0.020270 |\n",
      "| Epoch: 463/500 | Train Time: 0.008s | Train Loss: 0.023401 |\n",
      "| Epoch: 464/500 | Train Time: 0.009s | Train Loss: 0.022798 |\n",
      "| Epoch: 465/500 | Train Time: 0.008s | Train Loss: 0.027107 |\n",
      "| Epoch: 466/500 | Train Time: 0.008s | Train Loss: 0.023080 |\n",
      "| Epoch: 467/500 | Train Time: 0.008s | Train Loss: 0.022401 |\n",
      "| Epoch: 468/500 | Train Time: 0.007s | Train Loss: 0.026269 |\n",
      "| Epoch: 469/500 | Train Time: 0.007s | Train Loss: 0.025932 |\n",
      "| Epoch: 470/500 | Train Time: 0.007s | Train Loss: 0.021613 |\n",
      "| Epoch: 471/500 | Train Time: 0.008s | Train Loss: 0.025306 |\n",
      "| Epoch: 472/500 | Train Time: 0.008s | Train Loss: 0.021028 |\n",
      "| Epoch: 473/500 | Train Time: 0.008s | Train Loss: 0.024335 |\n",
      "| Epoch: 474/500 | Train Time: 0.008s | Train Loss: 0.027076 |\n",
      "| Epoch: 475/500 | Train Time: 0.008s | Train Loss: 0.020290 |\n",
      "| Epoch: 476/500 | Train Time: 0.008s | Train Loss: 0.024210 |\n",
      "| Epoch: 477/500 | Train Time: 0.008s | Train Loss: 0.024445 |\n",
      "| Epoch: 478/500 | Train Time: 0.008s | Train Loss: 0.029540 |\n",
      "| Epoch: 479/500 | Train Time: 0.007s | Train Loss: 0.024022 |\n",
      "| Epoch: 480/500 | Train Time: 0.008s | Train Loss: 0.026772 |\n",
      "| Epoch: 481/500 | Train Time: 0.008s | Train Loss: 0.024822 |\n",
      "| Epoch: 482/500 | Train Time: 0.008s | Train Loss: 0.020251 |\n",
      "| Epoch: 483/500 | Train Time: 0.008s | Train Loss: 0.024414 |\n",
      "| Epoch: 484/500 | Train Time: 0.008s | Train Loss: 0.025795 |\n",
      "| Epoch: 485/500 | Train Time: 0.007s | Train Loss: 0.028721 |\n",
      "| Epoch: 486/500 | Train Time: 0.007s | Train Loss: 0.024444 |\n",
      "| Epoch: 487/500 | Train Time: 0.008s | Train Loss: 0.027987 |\n",
      "| Epoch: 488/500 | Train Time: 0.007s | Train Loss: 0.025137 |\n",
      "| Epoch: 489/500 | Train Time: 0.008s | Train Loss: 0.024934 |\n",
      "| Epoch: 490/500 | Train Time: 0.008s | Train Loss: 0.031458 |\n",
      "| Epoch: 491/500 | Train Time: 0.009s | Train Loss: 0.023014 |\n",
      "| Epoch: 492/500 | Train Time: 0.008s | Train Loss: 0.023496 |\n",
      "| Epoch: 493/500 | Train Time: 0.008s | Train Loss: 0.022113 |\n",
      "| Epoch: 494/500 | Train Time: 0.007s | Train Loss: 0.023826 |\n",
      "| Epoch: 495/500 | Train Time: 0.008s | Train Loss: 0.025799 |\n",
      "| Epoch: 496/500 | Train Time: 0.008s | Train Loss: 0.022961 |\n",
      "| Epoch: 497/500 | Train Time: 0.008s | Train Loss: 0.022786 |\n",
      "| Epoch: 498/500 | Train Time: 0.007s | Train Loss: 0.025529 |\n",
      "| Epoch: 499/500 | Train Time: 0.009s | Train Loss: 0.025317 |\n",
      "| Epoch: 500/500 | Train Time: 0.008s | Train Loss: 0.025581 |\n",
      "Training Time: 5.463s\n",
      "Finished training.\n"
     ]
    }
   ],
   "source": [
    "deepSAD.train(dataset,\n",
    "              optimizer_name=config['optimizer_name'],\n",
    "              lr=config['lr'],\n",
    "              n_epochs=config['n_epochs'],\n",
    "              lr_milestones=config['lr_milestones'],\n",
    "              batch_size=config['batch_size'],\n",
    "              weight_decay=config['weight_decay'],\n",
    "              device=device,\n",
    "              n_jobs_dataloader=config['n_jobs_dataloader'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiQAAAD2CAYAAADmmpx9AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAxOAAAMTgF/d4wjAAAjgklEQVR4nO3de5zcdX3v8fd7L9lNssnmuoGwLAkJBJTGQAVKLTcP9Cg8bGkBORbrAdseOOe02qKPKtb2ILZaPUdKaY+CrYrYUil6Hl5KrUoBAUHBSCRggQRI1oTcyP22yW72c/74/oYdxuxuJuzMN7v7ej4e89iZ3/Xz+85vfvPe7+83M44IAQAA5NSQuwAAAAACCQAAyI5AAgAAsiOQAACA7AgkAAAgOwIJAADIjkACAACyI5AA45jtsH1B7jqqZftbtv80dx0ltnfZPq+K6Y+o+oEjgfliNKA2bD8g6eGI+HDuWgZjOyRdGBH35q6lnorwcL+k5ojoy1sNAIkeEgAYlO3m3DUA4wWBBMjAdqvtT9h+0fZW2w/ZPrNs/Btsf8/2tmL8UtuLinHn2/6R7e22N9v+vu3pQ6zrYts/KJaz2fZXKiY5uVjGLtvLbf9K2bzn2X6kmG+r7ftsL6kYH7Yvs/2c7Z22v2v7mLJp5tj+WrEtL9i+spjnvLJpzrT9QLGe1bY/artpiG16wPaflz0O238w2HZUzNsl6VvFw23F9B8qxq2y/RHb/2Z7p6T32T7a9r/Y3lBs35O2L69Y5iunvg6xTaqq33az7U/ZXm97k+2P237Y9g2DtREw2hBIgDz+t6SLJF0oaY6kr0m613ZnMf7Tkv5d0ixJsyX9jqRtxbh/kPR/JU2TdLSk90vaf7CV2L5Q0lckfapYzzGSPlMx2e9K+q/F8v5d0j+Wjestln+0pC5JKyV93faEimX8hqTTJXVKmiTpY2Xj7pTUKGm+pF+U9PaKGhcV6721qPEcSb8m6QMH26YhDLUdr4iIbklvLR5Oi4i2iCiv9xpJH5E0VdItRe2fl7RA0gxJfy3pTtuvH6aeodqk2vqvl3SJpHOL5e2TdKaAMYRAAtSZ7QalgPHhiFgZEfsj4lOSXpD0zmKy/UoB4LiI6IuIZRGxoWzcAklzi3kfjYjdg6zuvZI+FxF3F9P2RMS/V0zzqaKOPkl/J6nL9hxJiojvR8Qjxbw7lUJCl6RFFcu4PiK2R8R2pQByRrGtnZLeLOkDEbE1IrZK+pOKef+npG9GxJeLbV0t6ZOSrh6+NQ9tO6r0haJNIyL2RMSaiPh/EbErInoj4nOSflps11AO2iaHWf+7ivHPRsQ+SR+VtOUwtg04YhFIgPqbJWmipOcrhq9UerOXpKskhaT7bK+xfbPttmLcr0k6XtJS2yuLUwyDnd6YL+nZYep5qex+KdhMkSTbi21/0/Za2zskvViM7xhmGVOK+6XTFKvLxq+qmPcESb9RnNLZZnubUi/OUcPUfcjbUaUXyx/Ynm7775xOr+0o6nu9fr4NhqtnuFqGqv8YlbVhRByQ9LNhlgeMKgQSoP5eltSj1MtRboGkbkmKiNUR8XsRcZyk85RO7VxfjFseEb8VEUdJukzStRq8N2GVpBNfQ613KwWnUyJiqlLAkSQf4vxri7/HlQ07rmKa9ZLujIhpZbepEdGm2umvYtxfSjpJ6XRJe0RMk/S0Dr0NRsJalbVb0cvWOfjkwOhDIAFqq9HpAtZXbsXwz0u60fbxtifY/iNJC1VcN2D7Ktudti1ph6Q+SX3FtFfbnl0sZ7ukA8X4g/lrSb9j+9Ji3lbb/6mK+tuL9W+3PUPpWpRDFhFrJD0g6eO2p9meJunPKyb7tKTLbF9e1Nhoe6Htt1SzriqtL/5Wnno6mHZJeyRtltRs+w+Uekjq6UuS/sj2icX1Ox+WNLPONQA1RSABauuDkvZW3N6sdKHod5S+C2OjpEuVvg+k1A1/vqTHJO2S9BNJj0r6RDHuMklP294t6XuSbpf0xYOtPCK+I+kdRR2bJK1RumjzUL1b0uWSdkr6gQY+nVKN31LqTVgt6QlJ3yiG9xQ1Pq7UA/R7Sj0Bm5UuxK3sSRkxEfGcpL+RdH9xmuiDQ0z+YaVTbBuUepzmSPp+rWobxMcl3SPpIaU2mqjUlj11rgOoGb4YDUBdOX1s+Amli3LXZS5nVCquGVon6T0R8U+56wFGAj0kAGrK9im2T7PdUHzq5iZJ9xNGDp3tKbbfVpzSalPqLWvQ4fVYAUckAgmAWmuX9GWl0z5LlS7qvTJrRaNPg6Q/VWq7tUofIb4oIrblLAoYSZyyAQAA2dFDAgAAsiOQAACA7AgkAAAgu0F/TfNI1dLSErNnzx5+QgAAcMRYu3bt/ohoGWz8qAsks2fP1po1a3KXAQAAqmB701DjOWUDAACyI5AAAIDsRt0pGwAAjnT9/f0aj9/zZVsNDYfX10EgAQBghOzfv1/d3d3q7e3NXUo2zc3N6urq0oQJE6qaj0ACAMAI6e7u1pQpUzRz5kzZzl1O3UWENm/erO7ubi1cuLCqeWsaSGy3Kv2GxeuUfnZ9o6T/HhErbXdIukPSAkn7JP2PiHiwlvUM5cYbpZNPli6/PFcFAIDRrL+/X729vZo5c6aamsbv//szZ87Uli1b1N/fX9Xpm3pc1PpZSYsi4g2Svi7p74vhfynpBxFxgqSrJd1pu7kO9RzUT38qrVqVa+0AgNGudM3IeOwZKVfa/mqvoalpIImInoj41xio6geS5hX33y7p1mK6xyW9JOncWtYzlMZG6cCBXGsHAGB8q/fHft8r6eu2Z0pqjoj1ZeNWSeqqcz2vIJAAAMaaG264QT09PVXP99JLL+nss8+uQUWDq1sgsf0hSQslXV/lfNfZXlO67dq1qyb1EUgAAGPNRz7ykYMGkr6+viHnmzt3rh566KFalXVQdQkktt8v6TclvTUi9kTEZkl9to8qm2yepO7KeSPipojoLN3a2tpqUiOBBAAwllx77bWSpLPPPltLlizRRRddpHe/+90655xzdMopp0iSrrzySr3xjW/U4sWLdfHFF2v9+nTiYtWqVZo2bdory7Ktj33sYzrjjDM0f/58feELXxjxemseSGxfJ+kdki6MiG1lo+6WdG0xzemSjpH0vVrXMxgCCQBgJEVIO3bU7jbcNaO33nqrJOmhhx7SsmXL1NHRoaVLl+qee+7RM888I0m6+eab9aMf/UhPPvmkzj77bN1www2DLq+lpUWPPfaYvvWtb+k973nPsL0s1ar1x347JX1K0guS7i+uvN0XEWdK+oCkL9leIWm/pHdGRLZvkiGQAABG0s6dUnt77Za/fbs0dWp181x++eWaMmXKK4/vvPNOfelLX1JPT496eno0a9asQee98sorJUknnXSSmpqatH79enV2dh5W7QdT00ASEWskHfTzTxGxQdKv1nL91SCQAABG0pQpKTTUcvnVKr/s4eGHH9Ytt9yiRx99VB0dHfrGN76hP/uzPxt03tbW1lfuNzY2jq4ektGEQAIAGEl29T0YI23KlCnavn37q64HKdm6desr3yq7f/9+3XbbbfUvsAy/9lsgkAAAxpr3ve99uvDCC7VkyRJt3LjxVePe8pa3aNGiRVq0aNErF77m5NH2a4SdnZ2xZs2aEV/uH/+xtHev9Dd/M+KLBgCMAwcOHNBzzz2nE088UY2NjbnLyWawdrC9NiIGveiEHpJCU5M0wqfDAADAISKQFDhlAwBAPgSSAoEEAIB8CCQFAgkA4LU43F+5HWsO91eP+dhvgUACAHgtGhoa1NzcrM2bN2vmzJlVvyGPBRGhzZs3q7m5WQ0N1fV5EEgKBBIAwGvV1dWl7u5ubdmyJXcp2TQ3N6urq6vq+QgkBQIJAOC1mjBhghYuXKj+/v5xeerGdtU9IyUEkgKBBAAwUg73TXk8o8UKBBIAAPIhkBQIJAAA5EMgKRBIAADIh0BSIJAAAJAPgaRAIAEAIB8CSYFAAgBAPgSSAoEEAIB8CCSFpiapry93FQAAjE8EkgI9JAAA5EMgKRBIAADIh0BSIJAAAJAPgaRAIAEAIB8CSYFAAgBAPgSSAoEEAIB8CCQFAgkAAPkQSAoEEgAA8iGQFAgkAADkQyApEEgAAMiHQFIgkAAAkA+BpEAgAQAgn5oHEtu32F5lO2wvKRu+yvaztpcVtytqXctQCCQAAOTTVId1fEXSJyU9fJBxV0TEsjrUMCwCCQAA+dQ8kETEg5Jku9arek2amggkAADkkvsakjtsL7f9OduzcxbS2Cj19eWsAACA8StnIDknIhZLOk3Sy5K+eLCJbF9ne03ptmvXrpoUwykbAADyyRZIIqK7+Nsr6WZJZw8y3U0R0Vm6tbW11aQeAgkAAPlkCSS2J9ueVjboHZKeyFFLCYEEAIB8an5Rq+3bJF0s6ShJ37a9U9KvSvqq7UZJlvSCpHfVupahNDamv/39UkPuK2sAABhn6vEpm2sGGXVqrdddjVIgOXCAQAIAQL3x1lsoDyQAAKC+CCQFAgkAAPkQSAoEEgAA8iGQFAgkAADkQyApEEgAAMiHQFIgkAAAkA+BpEAgAQAgHwJJofTdIwQSAADqj0BSsFMoIZAAAFB/BJIyTU1SX1/uKgAAGH8IJGX4gT0AAPIgkJQhkAAAkAeBpAyBBACAPAgkZQgkAADkQSApQyABACAPAkkZAgkAAHkQSMoQSAAAyINAUoZAAgBAHgSSMgQSAADyIJCUIZAAAJAHgaQMgQQAgDwIJGUIJAAA5EEgKUMgAQAgDwJJGQIJAAB5EEjKNDVJfX25qwAAYPw55EBi+222pxb332/7K7ZPqV1p9UcPCQAAeVTTQ/IXEbHD9hskvVPSdyV9pjZl5UEgAQAgj2oCSelkxq9K+mxE3CZp8siXlA+nbAAAyKOaQNJo+0xJl0q6vxjWPPIl5dPSIu3bl7sKAADGn2oCyYcl3Sbp4Yj4D9uLJD1Xm7LyaG0lkAAAkEPToU4YEd+U9M2yx88q9ZaMGS0tUk9P7ioAABh/qvmUzY22pzm5x/bLtsdcIKGHBACA+qvmlM2vR8Q2SRcoXeD6JqXTOEOyfYvtVbbD9pKy4SfYfsT2c7Yft/36KmsfcZyyAQAgj2oCSX/x91xJdxenbOIQ5vuKpF+RtLpi+G1Kn9Y5UdInJN1eRS01wSkbAADyqCaQ7Lb9AUn/RdJ3bVvShOFmiogHI2JN+TDbHZLeKOkfikFflXSs7YVV1DPiOGUDAEAe1QSSqyQdLemPI2KDpAUaCBTVOlbSuojok6SICEndkroOc3kjorWVHhIAAHKo5lM2KyX9oe25tucWj/+ydqUltq+TdF3pcXt7e83WRQ8JAAB5VPMpm5NtPy3pKUlP215efBfJ4fiZpKNtNxXLtlLvSHflhBFxU0R0lm5tbW2HucrhEUgAAMijmlM2n1b6PZsZETFd0l9IuvVwVhoRGyX9WOk3caT0fSZril6XbDhlAwBAHtUEkukRcWfpQUR8WdL04WayfZvtNZI6JX3bdil0XCPpGtvPSfqgpKurqKUm6CEBACCPQ76GRNIB26+LiJ9Kku3XSRr2t3Ej4ppBhj8r6awq1l9zfA8JAAB5VBNIPiTpQdtPFo9/QdJ7R76kfPgeEgAA8qjmUzbftn2ypDOLQT+UtFTSnYPPNbpwygYAgDyq6SFRRGyS9C+lx8WnY8YMLmoFACCPai5qPZhD+er4UYMeEgAA8hi2h8T24iFGN49gLdkRSAAAyONQTtl8fYhxe0eqkCMBp2wAAMhj2EASEfPrUciRgB4SAADyeK3XkIwpBBIAAPIgkJQpnbKJMXWpLgAARz4CSZmWlhRG+vpyVwIAwPhCICnT2pr+ctoGAID6IpCUaWlJf/mkDQAA9UUgKVMKJPSQAABQXwSSMg0NUnMzPSQAANQbgaQCH/0FAKD+CCQVCCQAANQfgaQCXx8PAED9EUgqEEgAAKg/AkmFyZOl3btzVwEAwPhCIKnQ1ibt2pW7CgAAxhcCSYUpUwgkAADUG4GkQlubtHNn7ioAABhfCCQVOGUDAED9EUgqcMoGAID6I5BU4JQNAAD1RyCpwCkbAADqj0BSgVM2AADUH4GkAj0kAADUH4GkAteQAABQfwSSCpyyAQCg/ggkFThlAwBA/RFIKnDKBgCA+mvKuXLbqyTtk7S3GPTxiLgrX0WcsgEAIIesgaRwRUQsy11ESVubtG+f1NsrNTfnrgYAgPGBUzYV2trSX3pJAAConyMhkNxhe7ntz9menbuYSZMkm0ACAEA95Q4k50TEYkmnSXpZ0hcrJ7B9ne01pduuGieFhobUS7J9e01XAwAAymQNJBHRXfztlXSzpLMPMs1NEdFZurWVzqnUUEeH9PLLNV8NAAAoZAsktifbnlY26B2SnshUzqt0dEgbN+auAgCA8SPnp2zmSPqq7UZJlvSCpHdlrOcVHR3Shg25qwAAYPzIFkgi4gVJp+Za/1DoIQEAoL5yX9R6RCKQAABQXwSSgyCQAABQXwSSgyCQAABQXwSSgyCQAABQXwSSgyCQAABQXwSSg+jokHbskPbuHX5aAADw2hFIDmLWLKm1Veruzl0JAADjA4HkIBoapAULpJUrc1cCAMD4QCAZxMKF0ooVuasAAGB8IJAMYuFCekgAAKgXAskgCCQAANQPgWQQJ5xAIAEAoF4IJIM4+WTpxRel7dtzVwIAwNhHIBnE3LnS8cdLDz+cuxIAAMY+AskQzj9fuv/+3FUAADD2EUiGQCABAKA+CCRDOO88adkyaevW3JUAADC2EUiGcPTR0oknSg8+mLsSAADGNgLJMDhtAwBA7RFIhnHhhdI3vyn19+euBACAsYtAMoyLL5Z275a+853clQAAMHYRSIYxYYJ0zTXS3/5t7koAABi7mnIXMBpcc400f770wgvpy9IAAMDIoofkEMydK/36r0s33ZS7EgAAxiYCySG68Ubp85+Xli7NXQkAAGMPgeQQnXSS9Cd/Ir397dKWLbmrAQBgbCGQVOH666XFi6UrrpB6enJXAwDA2EEgqUJDg3THHdK2bdLs2XxhGgAAI4VAUqUpU6THHpM++UnpssvSKZyPflTq68tdGQAAoxeB5DDY0rXXSrffLp1xhnTXXdLrXiddcom0Zk3u6gAAGH0cEblrqEpnZ2esOcLe9bdskb797XT7p39KPScXXSRNnCi94Q3pO0wefTRN97a35a4WAID6s702IjoHHZ8zkNg+QdIXJc2StF3SVRHx9FDzHImBpNxzz0m33ir98IfS3r3SU0+lb3udMEGKkH73d9MP9rW3p7CyfXv6VeGGhjTezr0FAACMvCM9kNwn6Y6IuN32ZZI+EBGnDzXPkR5IKu3eLT3zTPp0ztNPp0/qrF4tbdokvfxymmbBgvSx4nvvlS69VPqlX5IefzyFmMWLpV/4hXQR7fHHp16WSZOkn/1MWrEihZrOztQbU2nz5vSjgLNnp8f79knNzSn8jLSdO9P1NYcqYuAHCxsbX9u6DxwYmeUAAGrniA0ktjskrZQ0IyL6bFvSOkm/EhErB5tvtAWSwUSkwDBxYjrV89RTqefk7rul5cul885LAWL58nTbvFnatWtg/tbWFFBWrJBaWqTp06UZM9Jy9+5Nb87PPCM1NUmnnZZCyI9/LM2aJXV0pI8t9/SkaQ8cSOFHSsuYMSP19CxcmMbt2iW1taXQMXduuq1encZ1dKRw9Nhj6ZeR+/tTPS0tqcbW1rQd99wjnXCCdOqp0rJl0sqVqba+vrQdTU0D27FnT7o2p78/bcfLL6eeo76+dGtuTvVOnJjG33WXtH+/9Mu/nMLahg1pmRs3phqfeioFNzst88QTpWOOkSZPTuP6+1P969enQNjYmG6lnq2NG1P7T5+etuXYY1O9O3e+ehnPPy+de27q/Zo9O7Xvnj2pjffulaZNS6HtxRfT8/HSS9Jxx6VTeq2taXnSwPZNn562d9Om1P47dqTnd9astK6urvTc7N2bpt2+PS1n1y6pt1f6+79PpwgXLEjBeMGCtM4dO9L279yZ1vP882m7tmyR3vzm1JZz56Z9YOJE6aijpDlzUvscOJBq7++Xpk5Nnzhrbk5t0t8vrVqVtn/DhlTnww+nbZ4/P7XHzJlpmc8+m8Y3NqZ5n3suPT8nn5y2Z9++NK6hIf1ta0vt19yc2mj58rS+0rDGxvSclELppz+dlnfllWlZXV3p+q7Vq1N98+alGjduTIF/5860vD170j8H/f3pOZ85M70enn8+1T9nTnrNtbendff2pvbq7U3P+bPPpueirS3VXrqtWpWex6OPTnXOmJH21QkTUjv29KTnaM+edGtvT8/1jBmpjSPSNJMnp1pKPatNTam91q9P+3Vvb9qWvr70eosYCP8R6bX0xBPpOZ00Ke1/pX8MNm9ObTZhQnotbtyYjiHHHz+wrXPnSlu3pjbfti3tR83NqZ4dO9K2T5+e6uzuTvVNmZKe8/K2mjRJuu++dIyZOzcto7l5YP/dvDndnzAhLWfixNT206en7Thw4NW30mt227a0/U1Nabr29jR+//60/Nmz0/6wc2eqV0rb1tqaai6NW7o0td/MmWm63t60v8yfn9bT3Jz239Jz1tKSnpcDB1Ktzc3SunXpNdfTk9p43ry0Pbt3p2Pm9Onpuf+rv5LOOks655y07bt3D3ySs3QMmTMn/Z0xI7VNT0/6R3TLlvSabm1N29zenp7brVultWvTdG1tad4pU9J+OG9eqv3AgVRXqQ3L78+bl7Z9pB3JgeQXJd0ZEYvKhj0m6YMRcd9g842VQFKtiHTQKb2g7PSijpCefDIdELZsSS+Wlpa0U597btrBH3kkTXfqqWnn3b17ICy0tqYD5v79aT2bNqWded689Ns9EyemF2qpB2TdunRgnzs3Dd+wIb2ozjhDeuCBdH///oHA09OTdvALLkjzPfVUeiGdckoaPmlSqqm3N01b2oYnnki19famF1npQNncnIb19qYD8f790pvelMatXj3wol25Mh0s9+9PvUwrVqTttKWf/CStZ9u2VMeECemNsK0tvYHaqbZ9+9L8HR3p4LFly8CBplT79u3SokWpnebOTdcK7d2bDhqTJqXhkyal52Tr1rTO+fPTG8axx6aD7YsvpvVMmZLW3dub1r11a3re5sxJy2xvT8/RunXp+XnppYGD6fbtqe337k3r6+mR3vpW6fvfT893S0s6GB11VJpn7dq0vp6e9IYzcWJa96OPpud106Y0vKcnvYm99FLavgkT0oHLTsNLPXObNqW/xx6b2mnOnDTsTW9K69q2LdVXCtYnnJCG7d2bln3SSWlb/+M/BtqtdJDs7x94oym9wZ5wQqpl27bUXpVvUOefn7b1n/85rfell9J+N39+eqNdty7VOHNmCurTpqXltbamfaexMQ3bujUtr709tcu6delNdPfu1Oal8NjUlF4LXV0DQb9Ue39/Wldn58C+U3pT6e1N2yOlZUyenLZ927bUxnv3pueuoWHgzbqrK72hlAJiS0vajhUrBtpOSm969sB+39CQln/aaWn9mzenmkvBr/QGtG9fuk2enIL8iy+mtm9uTq+xmTPT/tvenmro60vTTpuW6t66dSCgRaR59+1L21cKHjt3puPR+vUDz2Fv70A7TJs2UEdnZ3p9bNgwME05e6ANJ09O+3Vvb2r33bvTtk2YMHDMkFI7TZ2a2q8UOErBrKkpHTNWr07bMnVqWsYxx6R9oxSISrU0N6c26OpK40qha9asdAydOnXg9VLS0ZH2n3370r66YsXAByJaW9Px5uWX03M7eXLaZydMSNNPmjQQAidNGgguEQPLbGxM+1xLSzomlo7hRx2VjgO7dw+EuFLoL7//mc+kf85G2qgPJLavk3Rd6XF7e/sx27Ztq2epADIrBZHe3oFweqjzlXrXmgb5KdF9+9LyStdv9fWl+Zqb05tUKew1Ng69nN7eNK6a68D6+1PIKP1XXWnv3vQGVatry6q9bu1Qpi+9Kbe0DD5fqVejcr59+wYCxMGWu2fPz7+Blno+Dxx4dRuWeobKT1GXejMqn8NSICqFy5LK+Xt7B+resSO90Tc1DcxbqXx/2b17IEC2tKQ22rkz/QMlvfq0d3nIKu2/jY0peLS2DrRFW9vPt9H27Wk5R+Ip7CM5kIzrUzYAAIwnwwWSbN9DEhEbJf1Y0juLQZdKWjNUGAEAAGPTIJ2PdXONpNttf0jSDklXZ64HAABkkDWQRMSzks7KWQMAAMiPr44HAADZEUgAAEB2BBIAAJDdqPtxPdv7JG2q0eLbJO0adiq8VrRz/dDW9UE71wftXD+1aOvZEdEy2MhRF0hqyfaaoT4jjZFBO9cPbV0ftHN90M71k6OtOWUDAACyI5AAAIDsCCSvdlPuAsYJ2rl+aOv6oJ3rg3aun7q3NdeQAACA7OghAQAA2RFIAABAdgQSSbZPsP2I7edsP2779blrGq1s32J7le2wvaRs+KBtTPtXz3ar7a8VbfYT29+1vbAY12H732yvsP2U7XPK5ht0HA7O9ndsP2l7me2HbJ9aDGefrgHbVxfHj0uKx+zPI6w4Rj9b7NPLbF9RDM+7T0fEuL9Juk/SVcX9yyQ9nrum0XqTdI6kTkmrJC05lDam/Q+rnVslXaSB68B+X9IDxf3PS7qhuH+6pDWSmocbx23Qtp5Wdv83JP2kuM8+PfJtPU/SI5IelXRJMYz9eeTb+VXH57LhWffp7A2T+yapQ9IOSU3FY0taL2lh7tpG8618hx+qjWn/EWvvN0paVdzfJemosnGPSbpguHHcDqmdr5K0jH26Jm3bIOleSb8o6YGyQML+PPJt/XOB5EjYpzllIx0raV1E9ElSpNbultSVtaqxZag2pv1Hxnslfd32TKX/ENeXjVslqWuocXWrcpSyfYftn0n6qKTfFvt0LVwn6fsRsbQ0gP25pu6wvdz252zP1hGwTxNIgFHO9oeU/ou5PnctY1VEvCsijpX0YUmfyF3PWGP7FEmXSvrz3LWME+dExGJJp0l6WdIXM9cjiUAiST+TdLTtJkmybaXU1521qrFlqDam/V8D2++X9JuS3hoReyJis6Q+20eVTTZPUvdQ4+pV72gXEV+UdL7StQrs0yPnbKV9cYXtVZJ+SdJnJb1d7M8jLiK6i7+9km5Wav/sx+lxH0giYqOkH0t6ZzHoUklrImJlvqrGlqHamPY/fLavk/QOSRdGxLayUXdLuraY5nRJx0j63iGMQwXb02zPLXt8iaTNktinR1BEfCYijo6IeRExT9IPJP23iPiM2J9HlO3JtqeVDXqHpCeOhOM039QqyfYiSbdLmql04c7VEbE8a1GjlO3bJF0s6SilA/fOiFg4VBvT/tWz3an0X8sLknYWg/dFxJm250j6kqT5kvZL+v2IuL+Yb9Bx+Hm2j1N605soqV/SJknvj4hl7NO1Y/sBSTdHxNfYn0eW7eMlfVVSo9LFqS9Iem9ErMq9TxNIAABAduP+lA0AAMiPQAIAALIjkAAAgOwIJAAAIDsCCQAAyI5AAqBqxa+FLrF9le2TarSOG2y3lj2+0faVtVgXgPz42C+AqhXfpnmJ0rc83hwRX6ty/gZJioj+IaYJSdMrvvQNwBhFDwmAw3WB0q8M/5XtZbYvktLX2dt+zPaPbf9b8eVipR6Pr9r+tqSnlL6K+v/YfryY/8Hiy5dk+9ZiHQ8V4zps3277D4vxbbY/b/up4va/SkXZfqBY7kO2ny9bFoAjGIEEwOG6V9KPJP1RRCyJiH+1/VuSFkk6KyJOk/SPkj5dNs9Zkt4VEa+LiLWSPhERp0fEkmK6v5akiLi2mP7sYtkbK9b9p5JaJC2WdKakS2xfUTZ+gdJvzpwi6T/bPmvkNhtALTTlLgDAmHKJpNMlLU2/v6XGivH/GhEbyh5faPsPJE1R+gdpxiGu5wJJ7ytO+ey2fYekCyXdVYy/q/ip9D7by5QCyqPVbw6AeiGQABhJlvTxiPjsION3vTKh3SXpbyWdHhHP214s6cHDXG/lxXA9ZfcPiGMdcMTjlA2A12KHpPayx1+TdK3tGZJku9n2qYPM2y6pV9K64ufMf79i/M6KZZe7V9LvOJks6bclfefwNgHAkYBAAuC1+KykD5Uuao2If1T6RdD7bf9E0jJJbz7YjMUvhX5Z0tOSHpfUXTHJpyR9t3RRa8W4jyqFmeWSfijpGxHxzyOzSQBy4GO/AAAgO3pIAABAdgQSAACQHYEEAABkRyABAADZEUgAAEB2BBIAAJAdgQQAAGRHIAEAANkRSAAAQHb/H9lze06u964iAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x240 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss_moment(deepSAD.loss_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting testing...\n",
      "Test Loss: 0.262956\n",
      "Test AUC: 77.95%\n",
      "Test Time: 0.027s\n",
      "Finished testing.\n"
     ]
    }
   ],
   "source": [
    "# Test model\n",
    "deepSAD.test(dataset, device=device, n_jobs_dataloader=config['n_jobs_dataloader'])\n",
    "\n",
    "# Save results, model, and configuration\n",
    "deepSAD.save_results(export_json= config[\"export_path\"] + '/results.json')\n",
    "deepSAD.save_model(export_model=config[\"export_path\"] + '/model.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices, labels, scores = zip(*deepSAD.results['test_scores'])\n",
    "indices, labels, scores = np.array(indices), np.array(labels), np.array(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "result_df = pd.DataFrame()\n",
    "result_df['indices'] = indices\n",
    "result_df['labels'] = labels\n",
    "result_df['scores'] = scores\n",
    "\n",
    "label_normal = [0]\n",
    "label_abnormal = [1]\n",
    "\n",
    "result_df.drop('indices', inplace = True, axis = 1)\n",
    "df_normal = result_df[result_df.labels == 0]\n",
    "df_abnormal = result_df[result_df.labels == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_90 = df_normal.scores.quantile(0.90)\n",
    "y_90 = [1 if e > cut_90 else 0 for e in df_abnormal['scores'].values]\n",
    "\n",
    "cut_95 = df_normal.scores.quantile(0.95)\n",
    "y_95 = [1 if e > cut_95 else 0 for e in df_abnormal['scores'].values]\n",
    "\n",
    "cut_99 = df_normal.scores.quantile(0.99)\n",
    "y_99 = [1 if e > cut_99 else 0 for e in df_abnormal['scores'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary F1 Score: 0.6153846153846153\n",
      "Binary F1 Score: 0.3636363636363636\n",
      "Binary F1 Score: 0.13793103448275862\n"
     ]
    }
   ],
   "source": [
    "print(\"Binary F1 Score:\", f1_score(y_90, np.ones_like(y_90), average = 'binary'))\n",
    "print(\"Binary F1 Score:\", f1_score(y_95, np.ones_like(y_95), average = 'binary'))\n",
    "print(\"Binary F1 Score:\", f1_score(y_99, np.ones_like(y_99), average = 'binary'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.7794504181600955\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('AUC: {}\\n'.format(deepSAD.results['test_auc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('venvs': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9ecfe56d3cfcd43713b261054a7a1f07197771e57c2df97194a7291eee7dc423"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
